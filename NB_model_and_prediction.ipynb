{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics\n",
    "import unidecode\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airline_sentiment',\n",
       " 'is_reply',\n",
       " 'reply_count',\n",
       " 'retweet_count',\n",
       " 'text',\n",
       " 'tweet_coord',\n",
       " 'tweet_created',\n",
       " 'tweet_location',\n",
       " 'user_timezone',\n",
       " 'newairline_sentiment']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_public2.csv', encoding='utf-16', index_col='tweet_id', sep=',')\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_urlvideo(tweet):\n",
    "    #Delete URLs www.* or https?://*\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    #Delete VIDEO\n",
    "    tweet = re.sub('((\\[VIDEO\\])|(#VIDEO)|#vIDEO)','',tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_duplicated_tweets(df):\n",
    "    # Delete duplicated tweets\n",
    "    df['text'] = df['text'].apply(delete_urlvideo)\n",
    "    df2 = df # df.drop_duplicates(subset='text',keep=False) \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps:\n",
    "We create here a column named \"weekday_names\" in which we write the dat that the tweet was written\n",
    "We also create a column names \"hoursday\" in which we write the hour of the day in which the tweet was written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = remove_duplicated_tweets(df)\n",
    "weekdays = pd.to_datetime(df1.tweet_created).dt.dayofweek\n",
    "hoursday = pd.to_datetime(df1.tweet_created).dt.hour\n",
    "df1['hoursday'] = pd.Series(hoursday, index=df1.index)\n",
    "df1.tweet_created = pd.to_datetime(df1.tweet_created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hh = ['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']\n",
    "hh_names = ['zero', 'una', 'dos', 'tres', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve', 'diez', 'once', 'doce', 'trece', 'catorce', 'quince', 'dieciseis', 'diecisiete', 'dieciocho', 'diecinueve', 'veinte', 'veintiuno', 'veintios', 'veintitres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>newairline_sentiment</th>\n",
       "      <th>hoursday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>942743012337123328</th>\n",
       "      <td>positive</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Los pilotos de Ryanair desconvocan la huelga ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-12-18 13:07:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>positive</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   airline_sentiment  is_reply  reply_count  retweet_count  \\\n",
       "tweet_id                                                                     \n",
       "942743012337123328          positive     False            0              0   \n",
       "\n",
       "                                                                 text  \\\n",
       "tweet_id                                                                \n",
       "942743012337123328  \"Los pilotos de Ryanair desconvocan la huelga ...   \n",
       "\n",
       "                   tweet_coord       tweet_created tweet_location  \\\n",
       "tweet_id                                                            \n",
       "942743012337123328         NaN 2017-12-18 13:07:04            NaN   \n",
       "\n",
       "                   user_timezone newairline_sentiment  hoursday  \n",
       "tweet_id                                                         \n",
       "942743012337123328        Dublin             positive        13  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos la hora como HORAhora a los tweets. Así no tenemos que añadir la columna extra y podemos usar la función pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_con_hora = [' '.join((df1.text[index], 'HORA'+hh_names[df1.hoursday[index]])) for index in df1.index]\n",
    "df1.text = tweets_con_hora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to create the submit file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submit_file(df_submission, ypred):\n",
    "    date = datetime.datetime.now().strftime(\"%m_%d_%Y-%H_%M_%S\")\n",
    "    filename = 'submission_' + date + '.csv'\n",
    "\n",
    "    df_submission['airline_sentiment'] = ypred\n",
    "    df_submission[['airline_sentiment']].to_csv(filename)\n",
    "\n",
    "    print('Submission file created: {}'.format(filename))\n",
    "    print('Upload it to Kaggle InClass')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemlematizer\n",
    "This function takes the text in a tweet and reduces each word to its stem first, and then applies a lemmatization. As a result, a new dataframe is created, substituting the \"text\" column in the old one by the stemed-lematized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemlematizer(df):\n",
    "    \"\"\" Function to reduce the words to their root\n",
    "    \"\"\"\n",
    "    lemmer=WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "    new_corpus1 = [' '.join([stemmer.stem(word) for word in tweet.split(' ')]) for tweet in df.text.values]\n",
    "    new_corpus = [' '.join([lemmer.lemmatize(word) for word in tweet.split(' ')]) for tweet in new_corpus1] \n",
    "    \n",
    "    df.text = new_corpus\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This function is not used anymore in this notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_and_train(dataframe, trainmodelclassifier, test=None, *model_args, **model_kwargs ):\n",
    "\n",
    "# If there is no test data, split the input\n",
    "    if test is None:\n",
    "        train, test = train_test_split(dataframe, test_size=0.2)\n",
    "    else:\n",
    "        train = dataframe\n",
    "\n",
    "    dataframe.airline_sentiment = pd.Categorical(dataframe.airline_sentiment)\n",
    "    \n",
    "    x_train = train['text']\n",
    "    y_train = train['airline_sentiment']\n",
    "    \n",
    "    x_test = test['text']\n",
    "    #we create the list of stop words specific for spanish:\n",
    "    spanish_stop_words = stopwords.words('spanish')\n",
    "    #we build the classifier, that has everything inside:\n",
    "    text_clf = Pipeline([('vect', CountVectorizer(min_df=1,\n",
    "                                              stop_words=spanish_stop_words, ngram_range=(1,2), \n",
    "                                              analyzer='word', token_pattern=r'[^@]\\b\\w+\\b')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', trainmodelclassifier(*model_args, **model_kwargs))])\n",
    "\n",
    "    \n",
    "\n",
    "# SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None))])\n",
    "\n",
    "    text_clf.fit(x_train, y_train)\n",
    "    predicted = text_clf.predict(x_test)\n",
    "\n",
    "    try:\n",
    "        y_test = test['airline_sentiment'].values\n",
    "    except:\n",
    "        # It might be the submision file, where we don't have target values\n",
    "        y_test = None\n",
    "   \n",
    "    try:\n",
    "        acc = accuracy_score(test.airline_sentiment, predicted)\n",
    "        print('accuracy_score = ', acc)\n",
    "    except:\n",
    "        print('No accuracy computed beacuse there is NO test target')\n",
    "        acc = None\n",
    "    return predicted, x_train, y_train, x_test, y_test, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning functions:\n",
    "\n",
    "Those are a set of functions that clean a bit more the text: convert all into lower case, take the @ from usernames, take the # from hashtags, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processTweet2(tweet):\n",
    "    # process the tweets\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Remove usernames\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Remove #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', '', tweet)\n",
    "    #tweet = re.sub(r'\\#\\w+','',tweet)\n",
    "    return tweet  \n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    tweet = re.sub(r'CC:', ' EMOPOS ', tweet)\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMOPOS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMOPOS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMOPOS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMOPOS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMONEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMONEG ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # Remove punctuation\n",
    "    #word = word.strip('\\'\"?!,.():;')\n",
    "    word = word.strip('\\'\"?!,.')\n",
    "    # Convert more than 2 letter repetitions to 2 letter\n",
    "    # funnnnny --> funny\n",
    "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "    # Remove - & '\n",
    "    word = re.sub(r'(-|\\')', '', word)\n",
    "    # remove numbers\n",
    "    word = re.sub(r'\\d+','',word)\n",
    "    #remove users \n",
    "    word = re.sub(r'AT_USER','',word)\n",
    "    word = re.sub(r'URL','',word)\n",
    "    word = re.sub(r'rt','',word)\n",
    "    word = re.sub(r'via','',word)\n",
    "    word = re.sub('\\s+', ' ', word)\n",
    "    #Delete via, rt and by\n",
    "    word = re.sub(r'\\brt\\b','',word)\n",
    "    word = re.sub(r'\\bvia\\b','',word)\n",
    "    word = re.sub(r'\\bby\\b','',word)\n",
    "    # Remove numbers\n",
    "    word = re.sub(r'\\d+','',word)\n",
    "    # Remove single characters\n",
    "    word = re.sub(r'\\b\\w\\b','',word)\n",
    "    # Remove accents\n",
    "    word = unidecode.unidecode(word)\n",
    "\n",
    "    return word\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [handle_emojis(tweet) for tweet in df1.text.values]\n",
    "tweets2 = [processTweet2(tweet) for tweet in tweets]\n",
    "tweets3 = [' '.join(preprocess_word(word) for word in tweet.split(' ')) for tweet in tweets2]\n",
    "df1.text = tweets3\n",
    "df1.text.values\n",
    "df1 = stemlematizer(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the list of hour names, as strings, in order to fill it with the value of the hour at which the tweet was written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hh = ['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']\n",
    "hh_names = ['zero', 'una', 'dos', 'tres', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve', 'diez', 'once', 'doce', 'trece', 'catorce', 'quince', 'dieciseis', 'diecisiete', 'dieciocho', 'diecinueve', 'veinte', 'veintiuno', 'veintios', 'veintitres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we create the list of stop words specific for spanish:\n",
    "spanish_stop_words = stopwords.words('spanish')\n",
    "count_vect = CountVectorizer(min_df=1, stop_words=spanish_stop_words, ngram_range=(1,2), analyzer='word', token_pattern=r'[^@]\\b\\w+\\b')\n",
    "X_train_counts = count_vect.fit_transform(df1.text.values)\n",
    "voc = count_vect.vocabulary_\n",
    "X_train_counts = X_train_counts.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill the columns named as the hours with a 1 at the hour at which the tweet was written, and a 0 in all the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in hh_names:\n",
    "    ind = df1.query('hoursday == @i').index\n",
    "    df1.loc[ind,i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we copy these columns into the bag of words matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for h in hh_names:\n",
    "    col = np.array(df1[h].fillna(value = 0))\n",
    "    np.column_stack((X_train_counts,col))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the week days: first we create the columns with the names, and we fill them: we put 1 in the day at which the tweet was written, and 0 in all the other days. Then we copy these columns into the bag-of-words matrix. We do not use this feature anymore, since it was not adding any information to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for i in np.unique(np.array(df1['weekday_names'])):\n",
    "#    ind = df1.query('weekday_names == @i').index\n",
    "#    df1.loc[ind,i] = 1\n",
    "#for d in wn:\n",
    "#    col = np.array(df1[d].fillna(value = 0))\n",
    "#    np.column_stack((X_train_counts,col))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a Tfidf transformation in order to pass from absolute values to frequencies in the bag.of.words matrix. We then split the matrix into test and train and apply a classifier model, and test our training with the test sub-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.666018158236\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "features_train,features_test,label_train,label_test = \\\n",
    "train_test_split(X_train_tfidf, df1.newairline_sentiment.values,test_size=0.2,random_state = 1223)\n",
    "sgc = SGDClassifier(penalty = 'elasticnet',max_iter = 10)\n",
    "svm = LinearSVC()\n",
    "sgc.fit(features_train,label_train)\n",
    "svm.fit(features_train,label_train)\n",
    "score = sgc.score(features_test,label_test)\n",
    "#scoresvm = svm.score(features_test, label_test)\n",
    "print(score)\n",
    "#print(scoresvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell I try to compute the accuracy with another function (predict first, and then accuracy_score). The results are the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.664721141375\n"
     ]
    }
   ],
   "source": [
    "y_test_prediction = sgc.predict(features_test)\n",
    "acc = accuracy_score(label_test, y_test_prediction)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell I use the pipeline function load_and_train in order to compare it with the above result:\n",
    "\n",
    "In the above process, we add to the bag-of-words matrix some extra columns, as the day of the week or the hour of the day.\n",
    "In the next cell, these informations are not taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score =  0.613488975357\n"
     ]
    }
   ],
   "source": [
    "predicted, x_train, y_train, x_test, y_test, acc = load_and_train(df1, SGDClassifier, test=None, penalty='elasticnet', max_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the results are pretty much the same..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I do the processing of the tweets_submission.csv file, with the created model above. The procedure is exactly the same as before, except that here we use the bag-of-words created before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df0_s = pd.read_csv('tweets_submission.csv', index_col='tweet_id', sep=',')\n",
    "# Keep relevant columns only\n",
    "df1_s = df0_s.drop(['retweet_count','is_reply','reply_count','tweet_coord','tweet_location','user_timezone'], axis=1)\n",
    "df1_s['text'] = df1_s['text'].apply(delete_urlvideo)\n",
    "\n",
    "hoursday = pd.to_datetime(df1_s.tweet_created).dt.hour\n",
    "df1_s['hoursday'] = pd.Series(hoursday, index=df1_s.index)\n",
    "df1_s.tweet_created = pd.to_datetime(df1_s.tweet_created)\n",
    "\n",
    "tweets = [handle_emojis(tweet) for tweet in df1_s.text.values]\n",
    "tweets2 = [processTweet2(tweet) for tweet in tweets]\n",
    "tweets3 = [' '.join(preprocess_word(word) for word in tweet.split(' ')) for tweet in tweets2]\n",
    "df1_s.text = tweets3\n",
    "df1_s.text.values\n",
    "df1_s = stemlematizer(df1_s)\n",
    "\n",
    "count_vect = CountVectorizer(vocabulary=voc, min_df=1, stop_words=spanish_stop_words, ngram_range=(1,2), analyzer='word')\n",
    "X_submission_counts = count_vect.transform(df1_s.text.values)\n",
    "X_submission_counts = X_submission_counts.toarray()\n",
    "\n",
    "#for i in hh_names:\n",
    "#    ind = df1_s.query('hoursday == @i').index\n",
    "#    df1_s.loc[ind,i] = 1\n",
    "\n",
    "#for h in hh_names:\n",
    "#    col = np.array(df1_s[h].fillna(value = 0))\n",
    "#    np.column_stack((X_submission_counts,col))    \n",
    "    \n",
    "X_submission_tfidf = tfidf_transformer.transform(X_submission_counts)\n",
    "prediction_sgc = sgc.predict(X_submission_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission_06_14_2018-14_41_45.csv\n",
      "Upload it to Kaggle InClass\n"
     ]
    }
   ],
   "source": [
    "create_submit_file(df0_s, prediction_sgc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No accuracy computed beacuse there is NO test target\n"
     ]
    }
   ],
   "source": [
    "predicted, x_train, y_train, x_test, y_test, acc = load_and_train(df1, SGDClassifier, test=df1_s, penalty='elasticnet', max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission_06_14_2018-14_41_48.csv\n",
      "Upload it to Kaggle InClass\n"
     ]
    }
   ],
   "source": [
    "create_submit_file(df0_s, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy: 0.667315175097\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAEKCAYAAAB0cRxpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdc483c1320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.68      0.87      0.77       753\n",
      "    neutral       0.65      0.53      0.58       521\n",
      "   positive       0.63      0.35      0.45       268\n",
      "\n",
      "avg / total       0.66      0.67      0.65      1542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def plot_confusion_matrix(y, y_pred):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred),\n",
    "               cmap='inferno', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    plt.show()\n",
    "    \n",
    "print (\"classification accuracy:\", metrics.accuracy_score(label_test, y_test_prediction))\n",
    "plot_confusion_matrix(label_test, y_test_prediction)\n",
    "print(metrics.classification_report(label_test,y_test_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label well the negatives, but still label many neutral as negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>newairline_sentiment</th>\n",
       "      <th>...</th>\n",
       "      <th>catorce</th>\n",
       "      <th>quince</th>\n",
       "      <th>dieciseis</th>\n",
       "      <th>diecisiete</th>\n",
       "      <th>dieciocho</th>\n",
       "      <th>diecinueve</th>\n",
       "      <th>veinte</th>\n",
       "      <th>veintiuno</th>\n",
       "      <th>veintios</th>\n",
       "      <th>veintitres</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>942743012337123328</th>\n",
       "      <td>positive</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>los pilot de ryan desconvoc la huelg tras ver ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-12-18 13:07:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>positive</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926857871916183553</th>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>si por favor la declar de amor entre los  no ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-04 17:05:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936979305720090626</th>\n",
       "      <td>neutral</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>me diri por favor que cost tien</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-12-02 15:24:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943983853802328064</th>\n",
       "      <td>negative</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>champion no vuel ma con esos descar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-12-21 23:17:43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>negative</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938207464457211904</th>\n",
       "      <td>negative</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eso de anca e verd  mi mam le pa do vec terri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-12-06 00:44:25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>negative</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   airline_sentiment  is_reply  reply_count  retweet_count  \\\n",
       "tweet_id                                                                     \n",
       "942743012337123328          positive     False            0              0   \n",
       "926857871916183553          positive      True            0              0   \n",
       "936979305720090626           neutral      True            0              0   \n",
       "943983853802328064          negative      True            0              0   \n",
       "938207464457211904          negative      True            0              0   \n",
       "\n",
       "                                                                 text  \\\n",
       "tweet_id                                                                \n",
       "942743012337123328  los pilot de ryan desconvoc la huelg tras ver ...   \n",
       "926857871916183553   si por favor la declar de amor entre los  no ...   \n",
       "936979305720090626                    me diri por favor que cost tien   \n",
       "943983853802328064                champion no vuel ma con esos descar   \n",
       "938207464457211904   eso de anca e verd  mi mam le pa do vec terri...   \n",
       "\n",
       "                   tweet_coord       tweet_created tweet_location  \\\n",
       "tweet_id                                                            \n",
       "942743012337123328         NaN 2017-12-18 13:07:04            NaN   \n",
       "926857871916183553         NaN 2017-11-04 17:05:11            NaN   \n",
       "936979305720090626         NaN 2017-12-02 15:24:09            NaN   \n",
       "943983853802328064         NaN 2017-12-21 23:17:43            NaN   \n",
       "938207464457211904         NaN 2017-12-06 00:44:25            NaN   \n",
       "\n",
       "                                 user_timezone newairline_sentiment  \\\n",
       "tweet_id                                                              \n",
       "942743012337123328                      Dublin             positive   \n",
       "926857871916183553                         NaN             positive   \n",
       "936979305720090626                         NaN              neutral   \n",
       "943983853802328064  Central Time (US & Canada)             negative   \n",
       "938207464457211904  Eastern Time (US & Canada)             negative   \n",
       "\n",
       "                       ...      catorce  quince  dieciseis  diecisiete  \\\n",
       "tweet_id               ...                                               \n",
       "942743012337123328     ...          NaN     NaN        NaN         NaN   \n",
       "926857871916183553     ...          NaN     NaN        NaN         NaN   \n",
       "936979305720090626     ...          NaN     NaN        NaN         NaN   \n",
       "943983853802328064     ...          NaN     NaN        NaN         NaN   \n",
       "938207464457211904     ...          NaN     NaN        NaN         NaN   \n",
       "\n",
       "                    dieciocho  diecinueve  veinte  veintiuno  veintios  \\\n",
       "tweet_id                                                                 \n",
       "942743012337123328        NaN         NaN     NaN        NaN       NaN   \n",
       "926857871916183553        NaN         NaN     NaN        NaN       NaN   \n",
       "936979305720090626        NaN         NaN     NaN        NaN       NaN   \n",
       "943983853802328064        NaN         NaN     NaN        NaN       NaN   \n",
       "938207464457211904        NaN         NaN     NaN        NaN       NaN   \n",
       "\n",
       "                    veintitres  \n",
       "tweet_id                        \n",
       "942743012337123328         NaN  \n",
       "926857871916183553         NaN  \n",
       "936979305720090626         NaN  \n",
       "943983853802328064         NaN  \n",
       "938207464457211904         NaN  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
