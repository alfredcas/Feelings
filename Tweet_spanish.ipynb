{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import unidecode\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_public.csv', encoding='utf-16', index_col='tweet_id', sep=',')\n",
    "df_sub = pd.read_csv('tweets_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['airline_sentiment', 'is_reply', 'reply_count', 'retweet_count', 'text',\n",
      "       'tweet_coord', 'tweet_created', 'tweet_location', 'user_timezone'],\n",
      "      dtype='object')\n",
      "Index(['is_reply', 'reply_count', 'retweet_count', 'text', 'tweet_coord',\n",
      "       'tweet_created', 'tweet_id', 'tweet_location', 'user_timezone'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df_sub.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\"Los pilotos de Ryanair desconvocan la huelga tras ver reconocidos sus sindicatos\" by El País via Últimas noticias… https://t.co/80Fz6dxP9t',\n",
       "       '@Iberia @lavecinarubia Si ,por favor las declaraciones de amor entre los  #rubijarena no  pueden quedarse en una ma… https://t.co/GWKJGhhubY',\n",
       "       '@Iberia Me dirías por favor que costo tiene?', ...,\n",
       "       'Compré vuelos con @British_Airways. El vuelo es operado por @Iberia. Llamo a BA para añadir una maleta y me dicen q… https://t.co/HSUhcKH6Ie',\n",
       "       '@miguelitoelcon1 @Fjlopezm @Iberia Muchas gracias Miguel Ángel!',\n",
       "       'Ryanair abrirá en 2018 cuatro nuevas rutas desde Valencia, Alicante, Sevilla y Girona https://t.co/RzsnG7svht'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment    7867\n",
       "is_reply             7867\n",
       "reply_count          7867\n",
       "retweet_count        7867\n",
       "text                 7867\n",
       "tweet_coord            22\n",
       "tweet_created        7867\n",
       "tweet_location        439\n",
       "user_timezone        5119\n",
       "dtype: int64"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweet2(tweet):\n",
    "    # process the tweets\n",
    "\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub(r'@([^\\s]+)',r'\\1',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #tweet = re.sub(r'\\#\\w+','',tweet)\n",
    "    return tweet  \n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    tweet = re.sub(r'CC:', ' EMOPOS ', tweet)\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMOPOS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMOPOS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMOPOS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMOPOS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMONEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMONEG ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # Remove punctuation\n",
    "    #word = word.strip('\\'\"?!,.():;')\n",
    "    word = word.strip('\\'\"?!,.')\n",
    "    # Convert more than 2 letter repetitions to 2 letter\n",
    "    # funnnnny --> funny\n",
    "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "    # Remove - & '\n",
    "    word = re.sub(r'(-|\\')', '', word)\n",
    "    # remove numbers\n",
    "    word = re.sub(r'\\d+','',word)\n",
    "    #remove users \n",
    "    word = re.sub(r'AT_USER','',word)\n",
    "    word = re.sub(r'URL','',word)\n",
    "    word = re.sub(r'rt','',word)\n",
    "    word = re.sub(r'via','',word)\n",
    "    word = re.sub(r'by','',word)\n",
    "    return word\n",
    "\n",
    "def remove_accents(word):\n",
    "    word = unidecode.unidecode(word)\n",
    "    return word\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['los pilotos de ryanair desconvocan la huelga tras ver reconocidos sus sindicatos  el pais  ultimas noticias ',\n",
       "       'iberia lavecinarubia si por favor las declaraciones de amor entre los rubijarena no pueden quedarse en una ma ',\n",
       "       'iberia me dirias por favor que costo tiene', ...,\n",
       "       'compre vuelos con british_airways el vuelo es operado por iberia llamo a ba para anadir una maleta y me dicen q ',\n",
       "       'miguelitoelcon fjlopezm iberia muchas gracias miguel angel',\n",
       "       'ryanair abrira en  cuatro nuevas rutas desde valencia alicante sevilla y girona '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = [handle_emojis(tweet) for tweet in df.text.values]\n",
    "tweets2 = [processTweet2(tweet) for tweet in tweets]\n",
    "tweets3 = [' '.join(preprocess_word(remove_accents(word) )for word in tweet.split(' ')) for tweet in tweets2]\n",
    "df.text = tweets3\n",
    "df.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iberia esta clara vuestra politica de rascar por todos lados lo q os digo es q en vuelos de EUR de clientes fie ',\n",
       "       'iberia plus cumple  anos queremos celebrarlo contigo de una manera muy especial elige tu numero favorito y  ',\n",
       "       'a ver iberia de verdad lo vuestro con el espacio entre asientos es exagerado mido una mierda de .m y no qu ',\n",
       "       ...,\n",
       "       'samsungespana iberia despues de anos con samsung note  sedge tablet gear y ni flores  creo que empezare a ',\n",
       "       'mundo_ un pasajero de la aerolinea ryanair se hao de esperar y abrio la puea de emergencia para saltar al ala ',\n",
       "       'ultima hora: ofea ryanair vuelos a  euros   vuelosaeuro'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_sub = [handle_emojis(tweet) for tweet in df_sub.text.values]\n",
    "tweets2_sub = [processTweet2(tweet) for tweet in tweets_sub]\n",
    "tweets3_sub = [' '.join(preprocess_word(remove_accents(word) )for word in tweet.split(' ')) for tweet in tweets2_sub]\n",
    "df_sub.text = tweets3_sub\n",
    "df_sub.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(nltk.stem.SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['los pilot de ryan desconvoc la huelg tras ver reconoc sus sindicat  el pais  ultim notici',\n",
       "       'iberi lavecinarubi si por favor las declar de amor entre los rubijaren no pued qued en una ma',\n",
       "       'iberi me diri por favor que cost tien', ...,\n",
       "       'compr vuel con british_airways el vuel es oper por iberi llam a ba par anad una malet y me dic q',\n",
       "       'miguelitoelcon fjlopezm iberi much graci miguel angel',\n",
       "       'ryan abrir en  cuatr nuev rut desd valenci alic sevill y giron'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sno = nltk.stem.SnowballStemmer('spanish')\n",
    "tweet_1 = [' '.join(sno.stem(word) for word in tweet.split(' ')) for tweet in df.text.values]\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "tweet_2 = [' '.join(lemma.lemmatize(word,pos='v') for word in tweet.split(' ')) for tweet in tweet_1]\n",
    "df.text = [line.strip(' ') for line in tweet_2]\n",
    "df.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iberi esta clar vuestr polit de rasc por tod lad lo q os dig es q en vuel de eur de client fie',\n",
       "       'iberi plus cumpl  anos quer celebr contig de una maner muy especial elig tu numer favorit y',\n",
       "       'a ver iberi de verd lo vuestr con el espaci entre asient es exager mid una mierd de .m y no qu',\n",
       "       ...,\n",
       "       'samsungespan iberi despu de anos con samsung not  sedg tablet gear y ni flor  cre que empezar a',\n",
       "       'mundo_ un pasajer de la aeroline ryan se hao de esper y abri la pue de emergent par salt al ala',\n",
       "       'ultim hora: ofe ryan vuel a  eur   vuelosaeur'], dtype=object)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sno = nltk.stem.SnowballStemmer('spanish')\n",
    "tweet_1_sub = [' '.join(sno.stem(word) for word in tweet.split(' ')) for tweet in df_sub.text.values]\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "tweet_2_sub = [' '.join(lemma.lemmatize(word,pos='v') for word in tweet.split(' ')) for tweet in tweet_1_sub]\n",
    "df_sub.text = [line.strip(' ') for line in tweet_2_sub]\n",
    "df_sub.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7867, 56687)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,2),token_pattern=r'\\b\\w+\\b')\n",
    "X_train_counts = count_vect.fit_transform(df.text.values)\n",
    "voc = count_vect.vocabulary_\n",
    "print(X_train_counts.shape)\n",
    "X_train_counts = X_train_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7867, 56687)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_info(df):\n",
    "    # Time information transformed into hour of the day + day of the week one-hot encoding matrices\n",
    "    tweet_created = pd.to_datetime(df['tweet_created'])\n",
    "    hoursday = tweet_created.dt.hour\n",
    "    weekday = tweet_created.dt.dayofweek\n",
    "\n",
    "    # One-hot encoding for 'hoursday' and 'weekday'\n",
    "    hour1hot = pd.get_dummies(hoursday,prefix='h',columns=list(range(24)))\n",
    "    weekday1hot = pd.get_dummies(weekday,prefix='wd',columns=list(range(7)))\n",
    "    return hour1hot\n",
    "\n",
    "# Extract emojis from text\n",
    "from emoji import UNICODE_EMOJI #pip install emoji\n",
    "\n",
    "def emoji_shortname(patterns,thelist):\n",
    "    elist = []\n",
    "    for pattern in patterns:\n",
    "        for emoji in thelist:\n",
    "            match = re.search(pattern,emoji)\n",
    "            if match: elist.append(emoji)\n",
    "    return elist\n",
    "\n",
    "def get_emoji_img(mydict,emojiShortname):\n",
    "    ## Extract unicode emoji images based on the selected shortnames\n",
    "    return list(mydict.keys())[list(mydict.values()).index(emojiShortname)]\n",
    "\n",
    "def in_emoji(tweet,emojilist):\n",
    "    emo = 0\n",
    "    inemoji = []\n",
    "    for emoji in emojilist:\n",
    "        match = tweet.find(emoji)\n",
    "        if match != -1: inemoji.append(emoji)\n",
    "    if(len(inemoji) >=1): emo = 1\n",
    "    return emo \n",
    "\n",
    "def get_emoji(df):\n",
    "    # List of UNICODE emojis\n",
    "    unicodeemojilist = list(UNICODE_EMOJI.values())\n",
    "\n",
    "    # Regular expressions to cast negative emoji in the list of Unicode Emoji\n",
    "    negative_patterns = [re.compile(r'.*frown.*(face)'),re.compile(r'confounded'),re.compile(r'disappoint'),\n",
    "                     re.compile(r'worried'),re.compile(r'crying'),re.compile(r'\\bang[er][ry]'),re.compile(r'fear'),\n",
    "                     re.compile(r'weary'),re.compile(r'exploding_face'),re.compile(r'grimacing'),\n",
    "                     re.compile(r'face_with_steam_from_nose'),re.compile(r'pouting_face'),re.compile(r'sleepy_face'),\n",
    "                     re.compile(r'downcast_face_with_sweat'),re.compile(r'unamused_face'),\n",
    "                     re.compile(r'see-no-evil_monkey'),re.compile(r'pensive_face'),re.compile(r'persevering_face'),\n",
    "                     re.compile(r'anxi'),re.compile(r'scream'),re.compile(r'hot_face'),re.compile(r'flushed'),\n",
    "                     re.compile(r'zany_face'),re.compile(r'dizzy.*(face)'),re.compile(r'face_with_symbols_on_mouth'),\n",
    "                     re.compile(r'thumbs_down:'),re.compile(r'middle_finger:'),re.compile(r'broken_heart')]\n",
    "    # Regular expressions to cast positive emoji in the list of Unicode Emoji\n",
    "    positive_patterns = [re.compile(r'grin'),re.compile(r'joy'),re.compile(r'smil'),re.compile(r'kiss:'),re.compile(r'wink'),\n",
    "           re.compile(r'savoring_food'),re.compile(r'[^broken|couple_with]\\wheart'),re.compile(r'thumbs_up:'),\n",
    "           re.compile(r'OK_hand:'),re.compile(r'clapping_hands:'),re.compile(r'waving_hand:'),\n",
    "           re.compile(r'raised_hand:'),re.compile(r':relieved_face')]    \n",
    "                \n",
    "    # List of negative/positive emojis shortnames\n",
    "    negative_emoji_shortname = emoji_shortname(negative_patterns,unicodeemojilist)\n",
    "    positive_emoji_shortname = emoji_shortname(positive_patterns,unicodeemojilist)\n",
    "\n",
    "    # List of negative/positive emojis\n",
    "    nlist = []\n",
    "    for emoji in negative_emoji_shortname:\n",
    "        nlist.append(get_emoji_img(UNICODE_EMOJI,emoji)) \n",
    "    plist = []\n",
    "    for emoji in positive_emoji_shortname:\n",
    "        plist.append(get_emoji_img(UNICODE_EMOJI,emoji)) \n",
    "    \n",
    "    # Save the number of positive and negative emojis in two new columns\n",
    "    emopos = df['text'].apply(in_emoji,emojilist=plist)\n",
    "    emoneg = df['text'].apply(in_emoji,emojilist=nlist)\n",
    "    \n",
    "    emoji1hot = pd.DataFrame({'emopos':emopos,'emoneg':emoneg})\n",
    "    #emoji1hot.emopos.sum()\n",
    "    return emoji1hot\n",
    "\n",
    "import unidecode #pip install unidecode\n",
    "import string\n",
    "\n",
    "def basicCleaning(tweet):\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Delete URLs www.* or https?://*\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    # Remove accents\n",
    "    tweet = unidecode.unidecode(tweet)\n",
    "    #Delete via, rt and by\n",
    "    tweet = re.sub(r'\\b(rt|via|by)\\b','',tweet)\n",
    "    # Remove numbers\n",
    "    tweet = re.sub(r'\\d+','',tweet)\n",
    "    # Remove single characters\n",
    "    tweet = re.sub(r'\\b\\w\\b','',tweet)\n",
    "    # Remove email addresses\n",
    "    tweet = re.sub(r'\\w*@\\w*','',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('\\s+', ' ', tweet)\n",
    "    # Convert more than 2 letter repetitions to 2 letter\n",
    "    # funnnnny --> funny\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "    return tweet\n",
    "\n",
    "def moreCleaning(tweet):\n",
    "    #Delete @usernames\n",
    "    #tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    #Delete #hashtags\n",
    "    tweet = re.sub('#[^\\s]+','',tweet)\n",
    "    # Remove punctuation (includes @, \\ and #)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation+u\"¡\"+u\"¿\"+u\"€\"))\n",
    "    tweet = re.sub(regex,'',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('\\s+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def extract_airline(tweet):\n",
    "    airlinesList = ['aena','aeromar','aeromexico','aireuropa','airmadrid','airnostrum','americanairlines',\n",
    "         'avianca','blueair','britishairways','easyjet','emirates','iberia','klm',\n",
    "         'lufthansa','niki','norwegian','qatar','ryanair','spanair','spiritairlines',\n",
    "         'tame','vasp','vueling','westjet','wizzair']\n",
    "    patterns = [re.compile(r'aena'),re.compile(r'aeromar?'),\n",
    "            re.compile(r'aeromexi?c?o?'),re.compile(r'air\\s?europ?a?'),\n",
    "            re.compile(r'airmadr?i?d?'),re.compile(r'airnostru?m?'),\n",
    "            re.compile(r'american\\s?air?l?i?n?e?s?'),re.compile(r'avianca'),re.compile(r'blueai?r?'),\n",
    "            re.compile(r'british\\s?a?i?r?w?a?y?s?'),re.compile(r'easyjet'),\n",
    "            re.compile(r'emitares'),\n",
    "            re.compile(r'ibe?r?i?a?'),re.compile(r'klm'),\n",
    "            re.compile(r'lufthansa'),re.compile(r'niki'),re.compile(r'norwegian'),\n",
    "            re.compile(r'quatara?i?r?'),re.compile(r'ryanai?r?'),\n",
    "            re.compile(r'spanai?r?'),re.compile(r'spiritairl?i?n?e?s?'),\n",
    "            re.compile(r'tame'),re.compile(r'vasp'),\n",
    "            re.compile(r'vueling'),re.compile(r'westjet'),re.compile(r'wizza?i?r?')]\n",
    "      \n",
    "    noms = str()\n",
    "    i = 0\n",
    "    for airline in patterns:\n",
    "        match = re.search(airline, tweet)\n",
    "        if match : noms = noms+'|'+airlinesList[i]\n",
    "        i = i + 1\n",
    "    if (len(noms) == 0): \n",
    "        noms = 'noairline'\n",
    "    else:\n",
    "        noms = noms[1:]\n",
    "    return noms\n",
    "\n",
    "def cleaning_and_airlines(df):\n",
    "    airlinesList = ['aena','aeromar','aeromexico','aireuropa','airmadrid','airnostrum','americanairlines',\n",
    "         'avianca','blueair','britishairways','easyjet','emirates','iberia','klm',\n",
    "         'lufthansa','niki','noairline','norwegian','qatar','ryanair','spanair','spiritairlines',\n",
    "         'tame','vasp','vueling','westjet','wizzair']\n",
    "    textClean = df['text'].apply(basicCleaning)\n",
    "    airline = textClean.apply(extract_airline)\n",
    "\n",
    "    # From 'airline' column create a one-hot encoding matrix for airline name\n",
    "    tweet_airline = airline.str.split(r'|', expand=True).stack().reset_index(level='tweet_id')\n",
    "    tweet_airline.columns = ['tweet_id','airline']\n",
    "    tweet_airline = tweet_airline.set_index('tweet_id')\n",
    "    # One-hot encoding for airline name\n",
    "    onehot = pd.get_dummies(tweet_airline['airline'],columns=airlinesList)\n",
    "    #print(onehot.columns)\n",
    "    airlines1hot = onehot.groupby('tweet_id').sum()\n",
    "    #airlines1hot.sum() #Check the list of airlines extracted\n",
    "    return airlines1hot\n",
    "\n",
    "def get_1hot_hourAirlineEmoji(df):\n",
    "    hour1hot = get_time_info(df)\n",
    "    emoji1hot = get_emoji(df)\n",
    "    #airlines1hot = cleaning_and_airlines(df)\n",
    "    #print(hour1hot.shape,emoji1hot.shape,airlines1hot.shape)\n",
    "\n",
    "    # Concatenate the three 1-hot-encoding dataframes : hour + airline + emoji\n",
    "    merged = pd.concat([hour1hot,emoji1hot], axis=1, join_axes=[df.index])\n",
    "    #merged = pd.concat([hour1hot,emoji1hot], axis=1, join_axes=[df.index])\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tomerge = get_1hot_hourAirlineEmoji(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to term frequency matrix the [hour, airlines and emoji] frequency matrix\n",
    "all_matrix = np.column_stack((X_train_tfidf.toarray(),extra_tomerge.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7867, 56713)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7867, 56687)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7867, 26)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_tomerge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7867, 56713)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_reply(df):\n",
    "    if df:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "all_matrix_2 = np.c_[all_matrix,df['is_reply'].apply(is_reply).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7867, 56714)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_matrix_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6130397967823878\n",
      "0.6062658763759525\n"
     ]
    }
   ],
   "source": [
    "features_train,features_test,label_train,label_test = \\\n",
    "train_test_split(all_matrix_2, df.airline_sentiment.values,test_size=0.15,random_state = 123)\n",
    "sgc = SGDClassifier(penalty = 'elasticnet',max_iter = 10)\n",
    "svm = LinearSVC()\n",
    "sgc.fit(features_train,label_train)\n",
    "svm.fit(features_train,label_train)\n",
    "score = sgc.score(features_test,label_test)\n",
    "scoresvm = svm.score(features_test,label_test)\n",
    "print(score)\n",
    "print(scoresvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=10, n_iter=None,\n",
       "       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgc.fit(all_matrix_2, df.airline_sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(all_matrix_2,df.airline_sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(vocabulary=voc,ngram_range=(1,2),token_pattern=r'\\b\\w+\\b')\n",
    "X_test_counts = count_vect.fit_transform(df_sub.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_counts = X_test_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)\n",
    "extra_tomerge = get_1hot_hourAirlineEmoji(df_sub)\n",
    "all_matrix = np.column_stack((X_test_tfidf.toarray(),extra_tomerge.values))\n",
    "all_matrix_2 = np.c_[all_matrix,df_sub['is_reply'].apply(is_reply).values]\n",
    "prediction = svm.predict(all_matrix_2)\n",
    "prediction2 = sgc.predict(all_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission_06_14_2018-16_24_21.csv\n",
      "Upload it to Kaggle InClass\n",
      "Submission file created: submission_06_14_2018-16_24_27.csv\n",
      "Upload it to Kaggle InClass\n"
     ]
    }
   ],
   "source": [
    "def create_submit_file(df_submission, ypred):\n",
    "    date = datetime.now().strftime(\"%m_%d_%Y-%H_%M_%S\")\n",
    "    filename = 'submission_' + date + '.csv'\n",
    "\n",
    "    df_submission['airline_sentiment'] = ypred\n",
    "    df_submission[['tweet_id','airline_sentiment']].to_csv(filename,index_label = False,index = False)\n",
    "\n",
    "    print('Submission file created: {}'.format(filename))\n",
    "    print('Upload it to Kaggle InClass')\n",
    "#prediction[prediction == 0] = \"negative\"\n",
    "#prediction[prediction == 1] = \"neutral\"\n",
    "#prediction[prediction == 2] = \"positive\"\n",
    "create_submit_file(df_sub,prediction)\n",
    "time.sleep(5)\n",
    "create_submit_file(df_sub,prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
