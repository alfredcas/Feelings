{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Embedding,GlobalMaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D,Conv1D\n",
    "from keras.layers.convolutional import MaxPooling2D,MaxPooling1D\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import re\n",
    "import unidecode\n",
    "import nltk\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_info(df):\n",
    "    # Time information transformed into hour of the day + day of the week one-hot encoding matrices\n",
    "    tweet_created = pd.to_datetime(df['tweet_created'])\n",
    "    hoursday = tweet_created.dt.hour\n",
    "    weekday = tweet_created.dt.dayofweek\n",
    "\n",
    "    # One-hot encoding for 'hoursday' and 'weekday'\n",
    "    hour1hot = pd.get_dummies(hoursday,prefix='h',columns=list(range(24)))\n",
    "    weekday1hot = pd.get_dummies(weekday,prefix='wd',columns=list(range(7)))\n",
    "    return hour1hot\n",
    "\n",
    "# Extract emojis from text\n",
    "from emoji import UNICODE_EMOJI #pip install emoji\n",
    "\n",
    "def emoji_shortname(patterns,thelist):\n",
    "    elist = []\n",
    "    for pattern in patterns:\n",
    "        for emoji in thelist:\n",
    "            match = re.search(pattern,emoji)\n",
    "            if match: elist.append(emoji)\n",
    "    return elist\n",
    "\n",
    "def get_emoji_img(mydict,emojiShortname):\n",
    "    ## Extract unicode emoji images based on the selected shortnames\n",
    "    return list(mydict.keys())[list(mydict.values()).index(emojiShortname)]\n",
    "\n",
    "def in_emoji(tweet,emojilist):\n",
    "    emo = 0\n",
    "    inemoji = []\n",
    "    for emoji in emojilist:\n",
    "        match = tweet.find(emoji)\n",
    "        if match != -1: inemoji.append(emoji)\n",
    "    if(len(inemoji) >=1): emo = 1\n",
    "    return emo \n",
    "\n",
    "def get_emoji(df):\n",
    "    # List of UNICODE emojis\n",
    "    unicodeemojilist = list(UNICODE_EMOJI.values())\n",
    "\n",
    "    # Regular expressions to cast negative emoji in the list of Unicode Emoji\n",
    "    negative_patterns = [re.compile(r'.*frown.*(face)'),re.compile(r'confounded'),re.compile(r'disappoint'),\n",
    "                     re.compile(r'worried'),re.compile(r'crying'),re.compile(r'\\bang[er][ry]'),re.compile(r'fear'),\n",
    "                     re.compile(r'weary'),re.compile(r'exploding_face'),re.compile(r'grimacing'),\n",
    "                     re.compile(r'face_with_steam_from_nose'),re.compile(r'pouting_face'),re.compile(r'sleepy_face'),\n",
    "                     re.compile(r'downcast_face_with_sweat'),re.compile(r'unamused_face'),\n",
    "                     re.compile(r'see-no-evil_monkey'),re.compile(r'pensive_face'),re.compile(r'persevering_face'),\n",
    "                     re.compile(r'anxi'),re.compile(r'scream'),re.compile(r'hot_face'),re.compile(r'flushed'),\n",
    "                     re.compile(r'zany_face'),re.compile(r'dizzy.*(face)'),re.compile(r'face_with_symbols_on_mouth'),\n",
    "                     re.compile(r'thumbs_down:'),re.compile(r'middle_finger:'),re.compile(r'broken_heart')]\n",
    "    # Regular expressions to cast positive emoji in the list of Unicode Emoji\n",
    "    positive_patterns = [re.compile(r'grin'),re.compile(r'joy'),re.compile(r'smil'),re.compile(r'kiss:'),re.compile(r'wink'),\n",
    "           re.compile(r'savoring_food'),re.compile(r'[^broken|couple_with]\\wheart'),re.compile(r'thumbs_up:'),\n",
    "           re.compile(r'OK_hand:'),re.compile(r'clapping_hands:'),re.compile(r'waving_hand:'),\n",
    "           re.compile(r'raised_hand:'),re.compile(r':relieved_face')]    \n",
    "                \n",
    "    # List of negative/positive emojis shortnames\n",
    "    negative_emoji_shortname = emoji_shortname(negative_patterns,unicodeemojilist)\n",
    "    positive_emoji_shortname = emoji_shortname(positive_patterns,unicodeemojilist)\n",
    "\n",
    "    # List of negative/positive emojis\n",
    "    nlist = []\n",
    "    for emoji in negative_emoji_shortname:\n",
    "        nlist.append(get_emoji_img(UNICODE_EMOJI,emoji)) \n",
    "    plist = []\n",
    "    for emoji in positive_emoji_shortname:\n",
    "        plist.append(get_emoji_img(UNICODE_EMOJI,emoji)) \n",
    "    \n",
    "    # Save the number of positive and negative emojis in two new columns\n",
    "    emopos = df['text'].apply(in_emoji,emojilist=plist)\n",
    "    emoneg = df['text'].apply(in_emoji,emojilist=nlist)\n",
    "    \n",
    "    emoji1hot = pd.DataFrame({'emopos':emopos,'emoneg':emoneg})\n",
    "    #emoji1hot.emopos.sum()\n",
    "    return emoji1hot\n",
    "\n",
    "import unidecode #pip install unidecode\n",
    "import string\n",
    "\n",
    "def basicCleaning(tweet):\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Delete URLs www.* or https?://*\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    # Remove accents\n",
    "    tweet = unidecode.unidecode(tweet)\n",
    "    #Delete via, rt and by\n",
    "    tweet = re.sub(r'\\b(rt|via|by)\\b','',tweet)\n",
    "    # Remove numbers\n",
    "    tweet = re.sub(r'\\d+','',tweet)\n",
    "    # Remove single characters\n",
    "    tweet = re.sub(r'\\b\\w\\b','',tweet)\n",
    "    # Remove email addresses\n",
    "    tweet = re.sub(r'\\w*@\\w*','',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('\\s+', ' ', tweet)\n",
    "    # Convert more than 2 letter repetitions to 2 letter\n",
    "    # funnnnny --> funny\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "    return tweet\n",
    "\n",
    "def moreCleaning(tweet):\n",
    "    #Delete @usernames\n",
    "    #tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    #Delete #hashtags\n",
    "    tweet = re.sub('#[^\\s]+','',tweet)\n",
    "    # Remove punctuation (includes @, \\ and #)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation+u\"¡\"+u\"¿\"+u\"€\"))\n",
    "    tweet = re.sub(regex,'',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('\\s+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def extract_airline(tweet):\n",
    "    airlinesList = ['aena','aeromar','aeromexico','aireuropa','airmadrid','airnostrum','americanairlines',\n",
    "         'avianca','blueair','britishairways','easyjet','emirates','iberia','klm',\n",
    "         'lufthansa','niki','norwegian','qatar','ryanair','spanair','spiritairlines',\n",
    "         'tame','vasp','vueling','westjet','wizzair']\n",
    "    patterns = [re.compile(r'aena'),re.compile(r'aeromar?'),\n",
    "            re.compile(r'aeromexi?c?o?'),re.compile(r'air\\s?europ?a?'),\n",
    "            re.compile(r'airmadr?i?d?'),re.compile(r'airnostru?m?'),\n",
    "            re.compile(r'american\\s?air?l?i?n?e?s?'),re.compile(r'avianca'),re.compile(r'blueai?r?'),\n",
    "            re.compile(r'british\\s?a?i?r?w?a?y?s?'),re.compile(r'easyjet'),\n",
    "            re.compile(r'emitares'),\n",
    "            re.compile(r'ibe?r?i?a?'),re.compile(r'klm'),\n",
    "            re.compile(r'lufthansa'),re.compile(r'niki'),re.compile(r'norwegian'),\n",
    "            re.compile(r'quatara?i?r?'),re.compile(r'ryanai?r?'),\n",
    "            re.compile(r'spanai?r?'),re.compile(r'spiritairl?i?n?e?s?'),\n",
    "            re.compile(r'tame'),re.compile(r'vasp'),\n",
    "            re.compile(r'vueling'),re.compile(r'westjet'),re.compile(r'wizza?i?r?')]\n",
    "      \n",
    "    noms = str()\n",
    "    i = 0\n",
    "    for airline in patterns:\n",
    "        match = re.search(airline, tweet)\n",
    "        if match : noms = noms+'|'+airlinesList[i]\n",
    "        i = i + 1\n",
    "    if (len(noms) == 0): \n",
    "        noms = 'noairline'\n",
    "    else:\n",
    "        noms = noms[1:]\n",
    "    return noms\n",
    "\n",
    "def cleaning_and_airlines(df):\n",
    "    airlinesList = ['aena','aeromar','aeromexico','aireuropa','airmadrid','airnostrum','americanairlines',\n",
    "         'avianca','blueair','britishairways','easyjet','emirates','iberia','klm',\n",
    "         'lufthansa','niki','noairline','norwegian','qatar','ryanair','spanair','spiritairlines',\n",
    "         'tame','vasp','vueling','westjet','wizzair']\n",
    "    textClean = df['text'].apply(basicCleaning)\n",
    "    airline = textClean.apply(extract_airline)\n",
    "\n",
    "    # From 'airline' column create a one-hot encoding matrix for airline name\n",
    "    tweet_airline = airline.str.split(r'|', expand=True).stack().reset_index(level='tweet_id')\n",
    "    tweet_airline.columns = ['tweet_id','airline']\n",
    "    tweet_airline = tweet_airline.set_index('tweet_id')\n",
    "    # One-hot encoding for airline name\n",
    "    onehot = pd.get_dummies(tweet_airline['airline'],columns=airlinesList)\n",
    "    #print(onehot.columns)\n",
    "    airlines1hot = onehot.groupby('tweet_id').sum()\n",
    "    #airlines1hot.sum() #Check the list of airlines extracted\n",
    "    return airlines1hot\n",
    "\n",
    "def get_1hot_hourAirlineEmoji(df):\n",
    "    hour1hot = get_time_info(df)\n",
    "    emoji1hot = get_emoji(df)\n",
    "    #airlines1hot = cleaning_and_airlines(df)\n",
    "    #print(hour1hot.shape,emoji1hot.shape,airlines1hot.shape)\n",
    "\n",
    "    # Concatenate the three 1-hot-encoding dataframes : hour + airline + emoji\n",
    "    merged = pd.concat([hour1hot,emoji1hot], axis=1, join_axes=[df.index])\n",
    "    #merged = pd.concat([hour1hot,emoji1hot], axis=1, join_axes=[df.index])\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Feelings/tweets_public.csv', encoding='utf-16', index_col='tweet_id', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweet2(tweet):\n",
    "    # process the tweets\n",
    "\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #tweet = re.sub(r'\\#\\w+','',tweet)\n",
    "    return tweet  \n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    tweet = re.sub(r'CC:', ' EMOPOS ', tweet)\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMOPOS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMOPOS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMOPOS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMOPOS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMONEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMONEG ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # Remove punctuation\n",
    "    #word = word.strip('\\'\"?!,.():;')\n",
    "    word = word.strip('\\'\"?!,.')\n",
    "    # Convert more than 2 letter repetitions to 2 letter\n",
    "    # funnnnny --> funny\n",
    "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "    # Remove - & '\n",
    "    word = re.sub(r'(-|\\')', '', word)\n",
    "    # remove numbers\n",
    "    word = re.sub(r'\\d+','',word)\n",
    "    #remove users \n",
    "    word = re.sub(r'AT_USER','',word)\n",
    "    word = re.sub(r'URL','',word)\n",
    "    word = re.sub(r'rt','',word)\n",
    "    word = re.sub(r'via','',word)\n",
    "    word = re.sub(r'by','',word)\n",
    "    return word\n",
    "\n",
    "def remove_accents(word):\n",
    "    word = unidecode.unidecode(word)\n",
    "    return word\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
    "\n",
    "def is_reply(df):\n",
    "    if df:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [handle_emojis(tweet) for tweet in df.text.values]\n",
    "tweets2 = [processTweet2(tweet) for tweet in tweets]\n",
    "tweets3 = [' '.join(preprocess_word(remove_accents(word) )for word in tweet.split(' ')) for tweet in tweets2]\n",
    "df.text = tweets3\n",
    "sno = nltk.stem.SnowballStemmer('spanish')\n",
    "tweet_1 = [' '.join(sno.stem(word) for word in tweet.split(' ')) for tweet in df.text.values]\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "tweet_2 = [' '.join(lemma.lemmatize(word,pos='v') for word in tweet.split(' ')) for tweet in tweet_1]\n",
    "df.text = [line.strip(' ') for line in tweet_2]\n",
    "count_vect = CountVectorizer(ngram_range=(1,2),token_pattern=r'\\b\\w+\\b')\n",
    "X_train_counts = count_vect.fit_transform(df.text.values)\n",
    "voc = count_vect.vocabulary_\n",
    "X_train_counts = X_train_counts.toarray()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "extra_tomerge = get_1hot_hourAirlineEmoji(df)\n",
    "all_matrix = np.column_stack((X_train_tfidf.toarray(),extra_tomerge.values))\n",
    "all_matrix_2 = np.c_[all_matrix,df['is_reply'].apply(is_reply).values]\n",
    "features_train,features_test,label_train,label_test = \\\n",
    "train_test_split(all_matrix_2, df.airline_sentiment.values,test_size=0.15,random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6686, 50065) (1181, 50065)\n",
      "(6686,) (1181,)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape, features_test.shape)\n",
    "print(label_train.shape,label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50065"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(np.unique(label_train))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(label_train)\n",
    "label_train = encoder.transform(label_train)\n",
    "label_test = encoder.transform(label_test)\n",
    "print(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2000, input_dim=features_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2000,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1000,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(500,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6686 samples, validate on 1181 samples\n",
      "Epoch 1/10\n",
      "6686/6686 [==============================] - 327s 49ms/step - loss: 0.9583 - acc: 0.5208 - val_loss: 0.8651 - val_acc: 0.5876\n",
      "Epoch 2/10\n",
      "  50/6686 [..............................] - ETA: 5:47 - loss: 0.6203 - acc: 0.6600"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e4ead49a793f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(features_train,label_train, validation_data=(features_test,label_test), epochs = 10, batch_size=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(features_test,label_test,verbose = 1)\n",
    "print(100-scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all = encoder.transform(df.airline_sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.fit(all_matrix_2,label_all, epochs = 10, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(features_test,label_test,verbose = 1)\n",
    "print(100-scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv('../Feelings/tweets_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sub = [handle_emojis(tweet) for tweet in df_sub.text.values]\n",
    "tweets2_sub = [processTweet2(tweet) for tweet in tweets_sub]\n",
    "tweets3_sub = [' '.join(preprocess_word(remove_accents(word) )for word in tweet.split(' ')) for tweet in tweets2_sub]\n",
    "df_sub.text = tweets3_sub\n",
    "sno = nltk.stem.SnowballStemmer('spanish')\n",
    "tweet_1_sub = [' '.join(sno.stem(word) for word in tweet.split(' ')) for tweet in df_sub.text.values]\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "tweet_2_sub = [' '.join(lemma.lemmatize(word,pos='v') for word in tweet.split(' ')) for tweet in tweet_1_sub]\n",
    "df_sub.text = [line.strip(' ') for line in tweet_2_sub]\n",
    "count_vect = CountVectorizer(vocabulary=voc,ngram_range=(1,2),token_pattern=r'\\b\\w+\\b')\n",
    "X_test_counts = count_vect.fit_transform(df_sub.text.values)\n",
    "X_test_counts = X_test_counts.toarray()\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)\n",
    "extra_tomerge = get_1hot_hourAirlineEmoji(df_sub)\n",
    "all_matrix = np.column_stack((X_test_tfidf.toarray(),extra_tomerge.values))\n",
    "all_matrix_2 = np.c_[all_matrix,df_sub['is_reply'].apply(is_reply).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(all_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9999976e-01, 8.9718448e-08, 8.7793708e-08],\n",
       "       [2.1490701e-03, 5.6678727e-02, 9.4117224e-01],\n",
       "       [9.9999988e-01, 1.0547088e-07, 3.8558809e-08],\n",
       "       ...,\n",
       "       [9.9999738e-01, 2.5320796e-06, 1.7602167e-07],\n",
       "       [9.9999988e-01, 6.9616462e-08, 2.1587840e-10],\n",
       "       [8.0059353e-06, 9.9987352e-01, 1.1846994e-04]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = encoder.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission_06_14_2018-18_32_00.csv\n",
      "Upload it to Kaggle InClass\n"
     ]
    }
   ],
   "source": [
    "def create_submit_file(df_submission, ypred):\n",
    "    date = datetime.now().strftime(\"%m_%d_%Y-%H_%M_%S\")\n",
    "    filename = 'submission_' + date + '.csv'\n",
    "\n",
    "    df_submission['airline_sentiment'] = ypred\n",
    "    df_submission[['tweet_id','airline_sentiment']].to_csv(filename,index_label = False,index = False)\n",
    "\n",
    "    print('Submission file created: {}'.format(filename))\n",
    "    print('Upload it to Kaggle InClass')\n",
    "#prediction[prediction == 0] = \"negative\"\n",
    "#prediction[prediction == 1] = \"neutral\"\n",
    "#prediction[prediction == 2] = \"positive\"\n",
    "create_submit_file(df_sub,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
