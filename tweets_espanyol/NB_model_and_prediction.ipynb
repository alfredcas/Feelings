{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics\n",
    "import unidecode\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airline_sentiment',\n",
       " 'is_reply',\n",
       " 'reply_count',\n",
       " 'retweet_count',\n",
       " 'text',\n",
       " 'tweet_coord',\n",
       " 'tweet_created',\n",
       " 'tweet_location',\n",
       " 'user_timezone']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_public.csv', encoding='utf-16', index_col='tweet_id', sep=',')\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_urlvideo(tweet):\n",
    "    #Delete URLs www.* or https?://*\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    #Delete VIDEO\n",
    "    tweet = re.sub('((\\[VIDEO\\])|(#VIDEO)|#vIDEO)','',tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicated_tweets(df):\n",
    "    # Delete duplicated tweets\n",
    "    df['text'] = df['text'].apply(delete_urlvideo)\n",
    "    df2 = df.drop_duplicates(subset='text',keep=False) \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps:\n",
    "We create here a column named \"weekday_names\" in which we write the dat that the tweet was written\n",
    "We also create a column names \"hoursday\" in which we write the hour of the day in which the tweet was written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pere/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/pere/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/pere/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/pere/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:3643: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "df1 = remove_duplicated_tweets(df)\n",
    "weekdays = pd.to_datetime(df1.tweet_created).dt.dayofweek\n",
    "hoursday = pd.to_datetime(df1.tweet_created).dt.hour\n",
    "df1['weekday'] = pd.Series(weekdays, index=df1.index)\n",
    "df1['hoursday'] = pd.Series(hoursday, index=df1.index)\n",
    "weekday_names = []\n",
    "wn = ['lunes', 'martes', 'miercoles', 'jueves', 'viernes', 'sabado', 'domingo']\n",
    "for d in df1.weekday:\n",
    "    weekday_names.append(wn[d])\n",
    "df1['weekday_names'] = pd.Series(weekday_names, index=df1.index)\n",
    "df1.tweet_created = pd.to_datetime(df1.tweet_created)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to create the submit file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submit_file(df_submission, ypred):\n",
    "    date = datetime.datetime.now().strftime(\"%m_%d_%Y-%H_%M_%S\")\n",
    "    filename = 'submission_' + date + '.csv'\n",
    "\n",
    "    df_submission['airline_sentiment'] = ypred\n",
    "    df_submission[['airline_sentiment']].to_csv(filename)\n",
    "\n",
    "    print('Submission file created: {}'.format(filename))\n",
    "    print('Upload it to Kaggle InClass')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemlematizer\n",
    "This function takes the text in a tweet and reduces each word to its stem first, and then applies a lemmatization. As a result, a new dataframe is created, substituting the \"text\" column in the old one by the stemed-lematized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemlematizer(df):\n",
    "    \"\"\" Function to reduce the words to their root\n",
    "    \"\"\"\n",
    "    lemmer=WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "    new_corpus1 = [' '.join([stemmer.stem(word) for word in tweet.split(' ')]) for tweet in df.text.values]\n",
    "    new_corpus = [' '.join([lemmer.lemmatize(word) for word in tweet.split(' ')]) for tweet in new_corpus1] \n",
    "    \n",
    "    df.text = new_corpus\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This function is not used anymore in this notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_train(dataframe, trainmodelclassifier, test=None, *model_args, **model_kwargs ):\n",
    "\n",
    "# If there is no test data, split the input\n",
    "    if test is None:\n",
    "        train, test = train_test_split(dataframe, test_size=0.25)\n",
    "    else:\n",
    "        train = dataframe\n",
    "\n",
    "    dataframe.airline_sentiment = pd.Categorical(dataframe.airline_sentiment)\n",
    "    \n",
    "    x_train = train['text']\n",
    "    y_train = train['airline_sentiment']\n",
    "    \n",
    "    x_test = test['text']\n",
    "    #we create the list of stop words specific for spanish:\n",
    "    spanish_stop_words = stopwords.words('spanish')\n",
    "    #we build the classifier, that has everything inside:\n",
    "    text_clf = Pipeline([('vect', CountVectorizer(min_df=1,\n",
    "                                              stop_words=spanish_stop_words, ngram_range=(1,2), \n",
    "                                              analyzer='word', token_pattern=r'[^@]\\b\\w+\\b')),\n",
    "                     ('tfidf', TfidfTransformer(norm='l2')),\n",
    "                     ('clf', trainmodelclassifier(*model_args, **model_kwargs))])\n",
    "\n",
    "    \n",
    "\n",
    "# SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None))])\n",
    "\n",
    "    text_clf.fit(x_train, y_train)\n",
    "    predicted = text_clf.predict(x_test)\n",
    "\n",
    "    try:\n",
    "        y_test = test['airline_sentiment'].values\n",
    "    except:\n",
    "        # It might be the submision file, where we don't have target values\n",
    "        y_test = None\n",
    "   \n",
    "    try:\n",
    "        acc = accuracy_score(test.airline_sentiment, predicted)\n",
    "        print('accuracy_score = ', acc)\n",
    "    except:\n",
    "        print('No accuracy computed beacuse there is NO test target')\n",
    "        acc = None\n",
    "    return predicted, x_train, y_train, x_test, y_test, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning functions:\n",
    "\n",
    "Those are a set of functions that clean a bit more the text: convert all into lower case, take the @ from usernames, take the # from hashtags, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweet2(tweet):\n",
    "    # process the tweets\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Remove usernames\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Remove #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', '', tweet)\n",
    "    #tweet = re.sub(r'\\#\\w+','',tweet)\n",
    "    return tweet  \n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    tweet = re.sub(r'CC:', ' EMOPOS ', tweet)\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMOPOS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMOPOS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMOPOS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMOPOS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMONEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMONEG ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # Remove punctuation\n",
    "    #word = word.strip('\\'\"?!,.():;')\n",
    "    word = word.strip('\\'\"?!,.')\n",
    "    # Convert more than 2 letter repetitions to 2 letter\n",
    "    # funnnnny --> funny\n",
    "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "    # Remove - & '\n",
    "    word = re.sub(r'(-|\\')', '', word)\n",
    "    # remove numbers\n",
    "    word = re.sub(r'\\d+','',word)\n",
    "    #remove users \n",
    "    word = re.sub(r'AT_USER','',word)\n",
    "    word = re.sub(r'URL','',word)\n",
    "    word = re.sub(r'rt','',word)\n",
    "    word = re.sub(r'via','',word)\n",
    "    word = re.sub('\\s+', ' ', word)\n",
    "    #Delete via, rt and by\n",
    "    word = re.sub(r'\\brt\\b','',word)\n",
    "    word = re.sub(r'\\bvia\\b','',word)\n",
    "    word = re.sub(r'\\bby\\b','',word)\n",
    "    # Remove numbers\n",
    "    word = re.sub(r'\\d+','',word)\n",
    "    # Remove single characters\n",
    "    word = re.sub(r'\\b\\w\\b','',word)\n",
    "    # Remove accents\n",
    "    word = unidecode.unidecode(word)\n",
    "\n",
    "    return word\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pere/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:3643: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "tweets = [handle_emojis(tweet) for tweet in df1.text.values]\n",
    "tweets2 = [processTweet2(tweet) for tweet in tweets]\n",
    "tweets3 = [' '.join(preprocess_word(word) for word in tweet.split(' ')) for tweet in tweets2]\n",
    "df1.text = tweets3\n",
    "df1.text.values\n",
    "df1 = stemlematizer(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the list of hour names, as strings, in order to fill it with the value of the hour at which the tweet was written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hh = ['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']\n",
    "hh_names = ['zero', 'una', 'dos', 'tres', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve', 'diez', 'once', 'doce', 'trece', 'catorce', 'quince', 'dieciseis', 'diecisiete', 'dieciocho', 'diecinueve', 'veinte', 'veintiuno', 'veintios', 'veintitres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create the list of stop words specific for spanish:\n",
    "spanish_stop_words = stopwords.words('spanish')\n",
    "count_vect = CountVectorizer(min_df=1, stop_words=spanish_stop_words, ngram_range=(1,2), analyzer='word', token_pattern=r'[^@]\\b\\w+\\b')\n",
    "X_train_counts = count_vect.fit_transform(df1.text.values)\n",
    "voc = count_vect.vocabulary_\n",
    "X_train_counts = X_train_counts.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill the columns named as the hours with a 1 at the hour at which the tweet was written, and a 0 in all the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pere/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:357: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/Users/pere/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "for i in hh_names:\n",
    "    ind = df1.query('hoursday == @i').index\n",
    "    df1.loc[ind,i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we copy these columns into the bag of words matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in hh_names:\n",
    "    col = np.array(df1[h].fillna(value = 0))\n",
    "    np.column_stack((X_train_counts,col))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the week days: first we create the columns with the names, and we fill them: we put 1 in the day at which the tweet was written, and 0 in all the other days. Then we copy these columns into the bag-of-words matrix. We do not use this feature anymore, since it was not adding any information to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in np.unique(np.array(df1['weekday_names'])):\n",
    "#    ind = df1.query('weekday_names == @i').index\n",
    "#    df1.loc[ind,i] = 1\n",
    "#for d in wn:\n",
    "#    col = np.array(df1[d].fillna(value = 0))\n",
    "#    np.column_stack((X_train_counts,col))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a Tfidf transformation in order to pass from absolute values to frequencies in the bag.of.words matrix. We then split the matrix into test and train and apply a classifier model, and test our training with the test sub-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.616979269497\n",
      "0.606120434353\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "features_train,features_test,label_train,label_test = \\\n",
    "train_test_split(X_train_tfidf, df1.airline_sentiment.values,test_size=0.15,random_state = 123)\n",
    "sgc = SGDClassifier(penalty = 'elasticnet',max_iter = 10)\n",
    "svm = LinearSVC()\n",
    "sgc.fit(features_train,label_train)\n",
    "svm.fit(features_train,label_train)\n",
    "score = sgc.score(features_test,label_test)\n",
    "scoresvm = svm.score(features_test, label_test)\n",
    "print(score)\n",
    "print(scoresvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell I try to compute the accuracy with another function (predict first, and then accuracy_score). The results are the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.616979269497\n"
     ]
    }
   ],
   "source": [
    "y_test_prediction = sgc.predict(features_test)\n",
    "acc = accuracy_score(label_test, y_test_prediction)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell I use the pipeline function load_and_train in order to compare it with the above result:\n",
    "\n",
    "In the above process, we add to the bag-of-words matrix some extra columns, as the day of the week or the hour of the day.\n",
    "In the next cell, these informations are not taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pere/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:3643: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score =  0.613151658768\n"
     ]
    }
   ],
   "source": [
    "predicted, x_train, y_train, x_test, y_test, acc = load_and_train(df1, SGDClassifier, test=None, penalty='elasticnet', max_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the results are pretty much the same..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I do the processing of the tweets_submission.csv file, with the created model above. The procedure is exactly the same as before, except that here we use the bag-of-words created before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_s = pd.read_csv('tweets_submission.csv', index_col='tweet_id', sep=',')\n",
    "# Keep relevant columns only\n",
    "df1_s = df0_s.drop(['retweet_count','is_reply','reply_count','tweet_coord','tweet_location','user_timezone'], axis=1)\n",
    "df1_s['text'] = df1_s['text'].apply(delete_urlvideo)#df.head()\n",
    "#df_s['text'] = df_s['text'].apply(basicCleaning)\n",
    "#df_s['airline'] = df_s['text'].apply(extract_airline)\n",
    "# From 'airline' column create a one-hot encoding matrix for airline name\n",
    "#tweet_airline = df_s['airline'].str.split(r'|', expand=True).stack().reset_index(level='tweet_id')\n",
    "#tweet_airline.columns = ['tweet_id','airline']\n",
    "#tweet_airline = tweet_airline.set_index('tweet_id')\n",
    "# One-hot encoding for airline name\n",
    "#onehot = pd.get_dummies(tweet_airline['airline'])\n",
    "#df_airlines = onehot.groupby('tweet_id').sum()\n",
    "\n",
    "# Concatenate the two dataframes (original+one-hot airlines) and delete the airline column\n",
    "#merged = pd.concat([df_s, df_airlines], axis=1, join_axes=[df_s.index])\n",
    "#df1_s = merged.drop('airline', axis=1)\n",
    "#df1_s['text'] = df1_s['text'].apply(moreCleaning)\n",
    "\n",
    "weekdays = pd.to_datetime(df1_s.tweet_created).dt.dayofweek\n",
    "hoursday = pd.to_datetime(df1_s.tweet_created).dt.hour\n",
    "df1_s['weekday'] = pd.Series(weekdays, index=df1_s.index)\n",
    "df1_s['hoursday'] = pd.Series(hoursday, index=df1_s.index)\n",
    "weekday_names = []\n",
    "wn = ['lunes', 'martes', 'miercoles', 'jueves', 'viernes', 'sabado', 'domingo']\n",
    "for d in df1_s.weekday:\n",
    "    weekday_names.append(wn[d])\n",
    "df1_s['weekday_names'] = pd.Series(weekday_names, index=df1_s.index)\n",
    "df1_s.tweet_created = pd.to_datetime(df1_s.tweet_created)\n",
    "\n",
    "\n",
    "\n",
    "tweets = [handle_emojis(tweet) for tweet in df1_s.text.values]\n",
    "tweets2 = [processTweet2(tweet) for tweet in tweets]\n",
    "tweets3 = [' '.join(preprocess_word(word) for word in tweet.split(' ')) for tweet in tweets2]\n",
    "df1_s.text = tweets3\n",
    "df1_s.text.values\n",
    "df1_s = stemlematizer(df1_s)\n",
    "\n",
    "count_vect = CountVectorizer(vocabulary=voc, min_df=1, stop_words=spanish_stop_words, ngram_range=(1,2), analyzer='word')\n",
    "X_submission_counts = count_vect.fit_transform(df1_s.text.values)\n",
    "X_submission_counts = X_submission_counts.toarray()\n",
    "\n",
    "for i in hh_names:\n",
    "    ind = df1_s.query('hoursday == @i').index\n",
    "    df1_s.loc[ind,i] = 1\n",
    "\n",
    "for h in hh_names:\n",
    "    col = np.array(df1_s[h].fillna(value = 0))\n",
    "    np.column_stack((X_submission_counts,col))    \n",
    "    \n",
    "#for i in np.unique(np.array(df1_s['weekday_names'])):\n",
    "#    ind = df1_s.query('weekday_names == @i').index\n",
    "#    df1_s.loc[ind,i] = 1\n",
    "#for d in wn:\n",
    "#    col = np.array(df1_s[d].fillna(value = 0))\n",
    "#    np.column_stack((X_submission_counts,col))\n",
    "\n",
    "X_submission_tfidf = tfidf_transformer.transform(X_submission_counts)\n",
    "#features_train,features_test,label_train,label_test = \\\n",
    "#train_test_split(X_submission_tfidf, df1_s.airline_sentiment.values,test_size=1.00,random_state = 123)\n",
    "#sgc = SGDClassifier(penalty = 'elasticnet',max_iter = 10)\n",
    "#svm = LinearSVC()\n",
    "#sgc.fit(features_train,label_train)\n",
    "#svm.fit(features_train,label_train)\n",
    "prediction_sgc = sgc.predict(X_submission_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission_06_13_2018-15_08_57.csv\n",
      "Upload it to Kaggle InClass\n"
     ]
    }
   ],
   "source": [
    "create_submit_file(df0_s, prediction_sgc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pere/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:3643: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No accuracy computed beacuse there is NO test target\n"
     ]
    }
   ],
   "source": [
    "predicted, x_train, y_train, x_test, y_test, acc = load_and_train(df1, SGDClassifier, test=df1_s, penalty='elasticnet', max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission_06_11_2018-17_06_31.csv\n",
      "Upload it to Kaggle InClass\n"
     ]
    }
   ],
   "source": [
    "create_submit_file(df0_s, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy: 0.616979269497\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAEKCAYAAAB0cRxpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAH1BJREFUeJzt3Xu4XVV97vHvS+TihaugRi4CGkX0\nKGAOoHj6oNAWqRVboQdaNWhsxGKVVk8F9cEL2mIflaPVo0ahxlYFxAspxWpE8E4gYLiEaImIEoPE\nAAbiBd3Z7/ljjg2LzbrMvfdae+619/vxmc+al7Hm/O2l/jLmHHOMIdtERERn2zQdQETETJdEGRHR\nQxJlREQPSZQRET0kUUZE9JBEGRHRQyOJUtJuklZIurl87tqh3FZJq8uyfLrjjIgAUBPvUUr6Z+Au\n22dLOh3Y1fab2pTbYvtR0x5gRESLphLlD4Ejbd8uaT5whe2ntCmXRBkRjWsqUf7S9i4t23fbfsjt\nt6QRYDUwApxt+0sdzrcEWALwyEdu/6wDDnj8YAKfBW78/l1NhzDj3Te6uekQhsEm23tM5QR/fMwz\nfOemLbXKXnPNj79i+5ipXG8qHjaoE0v6GvC4NofeMoHT7GN7g6T9ga9LusH2j8YXsr0UWAqwcOH+\nXnn1WZOKeS44YMcLmw5hxlv3q/9sOoQhsPUnUz3DnZu2sHJVvf+vPkwv3X2q15uKgSVK20d3Oibp\nDknzW269N3Y4x4byeYukK4CDgYckyogYPsaMjm5tOoxamno9aDmwqKwvAi4eX0DSrpK2L+u7A0cA\nN01bhBExWDajo/fVWpo2sBplD2cDF0paDPwUOAFA0kLgFNuvAp4KfEzSKFVCP9t2EmXELGHMqEea\nDqOWRhKl7TuBo9rsXwW8qqx/F/gf0xxaREwb4yTKiIhukigjIrqz8WgSZUREd6lRRkR0M4q3/qbp\nIGpJooyIRth5RhkR0YMhzygjIrpwEmVERG+59Y6I6EweRSO/bTqMWpIoI6IhufWOiOjBKLfeERFd\nGBiSYdaSKCOiIUZDcuud6WojoiGuapR1lhokzZP0fUmXlO39JK0ss71eIGm7sn/7sr2uHN+317mT\nKCOiGTYaua/WUtPrgbUt2+8BzrG9ALgbWFz2Lwbutv0k4JxSrqskyohohvtXo5S0F/AnwCfKtoDn\nAxeVIsuAF5f148o25fhRpXxHeUYZEY1R/cac3SWtatleWiYVHPN/gX8AdizbjwZ+6Qc6k68H9izr\newK3AdgekbS5lN/U6eJJlBHREE+k1XuT7YXtDkh6IbDR9jWSjhzb3f6CPY+1lUQZEY2QPZEaZTdH\nAC+SdCywA7ATVQ1zF0kPK7XKvYANpfx6YG9gvaSHATsDXSe8zzPKiGiGjUZ+V2vpfhqfYXsv2/sC\nJwJft/1XwOXA8aVY62yvrbPAHl/Kp0YZETPUYF84fxNwvqR3Ad8Hzi37zwX+TdI6qprkib1OlEQZ\nEQ0xGh3t7xntK4AryvotwKFtyvyWMkV2XUmUEdGMdGGMiOhlQq3ejUqijIjGyP299R6UJMqIaIYN\nI79vOopakigjohk29LkxZ1AafY9S0jGSflhG8Ti9zfEJj/IREcNDo1trLU1rLFFKmgd8GHgBcCBw\nkqQDxxWb8CgfETEsSo2yztKwJmuUhwLrbN9i+3fA+VSjerSa8CgfETEkTBJlDfeP4FG0ju7xkDKl\nv+bYKB8RMfSGp0bZZGNOnRE8ao3yIWkJsARgn32SRyOGgWw0JK3eTdYox0bwGNM6usdDynQb5cP2\nUtsLbS/cY4+dBhRuRPTdkNQom0yUVwMLyrwW21F1TF8+rsyER/mIiCExRM8oG7v1LiMLvxb4CjAP\nOM/2GknvBFbZXs4kRvmIiGExPO9RNvrCue1LgUvH7TuzZX3Co3xExJAwMDocN4jpmRMRDTGMDMe8\n3kmUEdGMIapRZiqIiGiOR+stXUjaQdJVkq6TtEbSO8r+T0r6saTVZTmo7JekD5au0ddLOqRXmKlR\nRkRD3K8a5X3A821vkbQt8G1JXy7H/o/ti8aVfwGwoCyHAR8pnx0lUUZEM/p0611eGdxSNrctS7cT\nHwd8qnzvSkm7SJpv+/ZOX8itd0Q0Z9T1Fthd0qqWZUnraSTNk7Qa2AissL2yHHp3ub0+R9L2ZV+d\n7tMPkhplRDTCBo/UrlFusr2w87m8FThI0i7AFyU9HTgD+DmwHbCUalbGd1Kza3Sr1CgjohkGRmsu\ndU9p/5JqFsZjbN/uyn3Av/LAjIx1uk8/SBJlRDSnD4lS0h6lJomkhwNHAz+QNL/sE/Bi4MbyleXA\ny0vr9+HA5m7PJyG33hHRpP68RjkfWFYGA98GuND2JZK+LmkPqlvt1cAppfylwLHAOuDXwCt6XSCJ\nMiKaYfDo1Mfhtn09cHCb/c/vUN7AqRO5RhJlRDRnOMbESKKMiIYYPDIczSRJlBHREEEfbr2nQxJl\nRDTHSZQREZ31qTFnOiRRRkRzRvOMMiKiMwtvTaKMiOguNcqIiM6cZ5QREb3k9aCIiJ6c14MiIrow\neUYZEdGdGE2rd0REF6lRRkT0llbviIguzPA05gxHvTciZh+ruvWus3QhaQdJV0m6TtIaSe8o+/eT\ntFLSzZIukLRd2b992V5Xju/bK9RGE6WkYyT9sAR8epvjJ0v6haTVZXlVE3FGxGB4VLWWHu4Dnm/7\nmcBBwDFlLpz3AOfYXgDcDSwu5RcDd9t+EnBOKddVY4myzG/xYeAFwIHASZIObFP0AtsHleUT0xpk\nRAyOhbfOq7V0PU1lS9nctiwGng9cVPYvo5pgDOC4sk05flSZgKyjJmuUhwLrbN9i+3fA+VR/QETM\nEROoUe4uaVXLsqT1PJLmSVoNbARWAD8Cfml7pBRZD+xZ1vcEbgMoxzcDj+4WZ5ONOfcHW6wHDmtT\n7iWS/gD4b+DvbN82vkD50ZYA7DhvR96018oBhDs7HLH9vk2HMOP95L49mg5hxvv9yM+nfI4JNuZs\nsr2w47nsrcBBZdraLwJP7XBJqGZl7HSsrSZrlHWC/Q9gX9vPAL7GA9XlB3/JXmp7oe2Fj9jm4X0O\nMyIGwn17RvnAKe1fAlcAhwO7SBqrDO4FbCjr64G9AcrxnYG7up23yUR5f7BF6x8CgO07bd9XNj8O\nPGuaYouIgRP2NrWWrmeR9ig1SSQ9HDgaWAtcDhxfii0CLi7ry8s25fjXyxS2HTV56301sEDSfsDP\ngBOBv2wtIGm+7dvL5ouo/viImCX6NHDvfGBZaSDeBrjQ9iWSbgLOl/Qu4PvAuaX8ucC/SVpHVZM8\nsdcFGkuUtkckvRb4CjAPOM/2GknvBFbZXg68TtKLgBGqP+jkpuKNiD7r03iUtq8HDm6z/xaqRuPx\n+38LnDCRazTaM8f2pcCl4/ad2bJ+BnDGdMcVEYPncus9DNKFMSIak77eERHdeHj6eidRRkRjkigj\nIrowYrRH98SZIokyIpqRWRgjInobllvvWm3zkp4r6RVlfY/yknhExJTYqrU0rWeNUtLbgIXAU4B/\npRrC6N+BIwYbWkTMap5YP+4m1bn1/jOqt96vBbC9QdKOA40qImY9A6Ojs6cx53e2LckAkh454Jgi\nYo4YnQG31XXUSZQXSvoY1ZBFfw28kmokn4iIyZtNt9623yvpD4F7qJ5Tnml7xcAji4hZbZhmYaz1\nelBJjEmOEdFXsyZRSrqXB0Ye346q1ftXtncaZGARMfvNmkRp+0Et3JJeTJsx3iIiJsIWW4ekC+OE\nB4Oz/SWqaSAjIqakHy+cS9pb0uWS1kpaI+n1Zf/bJf1M0uqyHNvynTMkrZP0Q0l/3CvOOrfef96y\nuQ3Vy+dd55eIiKijT7feI8AbbF9b3vG+RtJYm8o5tt/bWljSgVTTPzwNeDzwNUlPLjM5tlWnMedP\nxwV0K5l/OyKmyv15j7LMq3V7Wb9X0loemMO7neOA88vEhT8uc+ccCnyv0xfqPKN8xYSijoiooZoK\nor+NOZL2pepJuJKqm/VrJb0cWEVV67ybKole2fK19XRPrJ0TpaR/ocsttu3X1Yw9IqKtCSTK3SWt\natleantpawFJjwI+D5xm+x5JHwHOospjZwHvo+ow0+6ik56udlWXYxERU7Z1tHZ78ibbCzsdlLQt\nVZL8tO0vANi+o+X4x4FLyuZ6YO+Wr+8FbOh28Y6J0vaynqFHREyS+zRnjiRRzdW91vb7W/bPL88v\noRrc58ayvhz4jKT3UzXmLACu6naNOq3eewBvAg4EdhjbbzuvCEXEFKhfg2IcAbwMuEHS6rLvzcBJ\nkg6iuq2+FXg1gO01ki4EbqJqoD61W4s31Gv1/jRwAfAnwCnAIuAXE/5TIiLG6UeN0va3af/c8dIu\n33k38O6616jzgODRts8Ffm/7G7ZfCRxe9wIREZ3MmhHOgd+Xz9sl/QnVQ8+9BhdSRMwF9oQacxpV\nJ1G+S9LOwBuAfwF2Av5uoFFFxBzQt2eUA1cnUa60vRnYDDyvnxeXdB7wQmCj7ae3OS7gA8CxwK+B\nk21f288YIqIZpnrpfBjUqfd+V9JXJS2WtGufr/9J4Jgux19A1XS/AFgCfKTP14+IBg3LM8qeidL2\nAuCtVB3Ir5F0iaSX9uPitr8J3NWlyHHAp1y5kmo6ivn9uHZENG/UqrU0rdaTVNtX2f57qo7jdwHT\n9TL6nsBtLdtt+2RKWiJplaRVvx79zTSFFhFTU682ORNqlHVeON+J6q32E4EnAl9k+gburdUns/T5\nXArwuO0emyHgIobAbGv1vg74EvBO2x2HIRqQCffJjIjhMTokjTl1EuX+tpuqpS2nGibpfOAwYHNL\n382IGGKzahbGQSZJSZ8FjqQaQmk98Daqycuw/VGqLkjHAuuoXg/K2JgRs8bMaKipo9Z0tYNi+6Qe\nxw2cOk3hRMQ0mzU1yoiIQTAw2nQQNfVscpL0ZEmXSbqxbD9D0lsHH1pEzGql1bvO0rQ6EXwcOIMy\nOIbt66leFYqImBKjWkvT6tx6P8L2VVW36/uNDCieiJgjPMsaczZJeiLlRW9Jx1OmhoyImIrRIeke\nUufW+1TgY8ABkn4GnAa8ZqBRRcSc0I9bb0l7S7pc0lpJayS9vuzfTdIKSTeXz13Lfkn6oKR1kq6X\ndEivOOu8R3kLcLSkRwLb2L631i8QEdFF1YWxL7feI1Rzdl8raUeqwXtWACcDl9k+W9LpwOlU83+1\njkp2GNWoZId1u0Cdvt5njtsGwPY7J/rXRES06kcXxtJb7/ayfq+ktVSD5xxH1aEFqoF8rqBKlPeP\nSgZcKWmXcTM2PkSdZ5S/alnfgWqg3bUT+1MiIh5sgl0Yd5e0qmV7aRkM50Ek7QscDKwEHjuW/Gzf\nLukxpVinUckmnyhtv29cIO+l6oMdETEFE2r13mR7YdezSY8CPg+cZvuecW/qPPjCD9W1WWkyb3I+\nAth/Et+LiHgQ11x6kbQtVZL8tO0vlN13jA30XT43lv0THpWsTs+cG0rL0PWS1gA/pJrHJiJi0kx/\nRjgvc2udC6y1/f6WQ8uBRWV9EXBxy/6Xl9bvw6kxKlmdZ5QvbFkfAe6wnRfOI2LKtvbnhfMjgJcB\nN0haXfa9GTgbuFDSYuCnwAnl2IRHJeuaKCVtA/xnuxkSIyKmwqYvPXNsf5v2zx0BjmpTfsKjknW9\n9bY9ClwnaZ+JnDQioo5+PaMctDq33vOBNZKuouVVIdsvGlhUETEnzKa+3u8YeBQRMecM03iUdRLl\nsbbf1LpD0nuAbwwmpIiYG2bGVLR11HmP8g/b7HtBvwOJiLnFVK3edZamdaxRSnoN8DfA/pKubzm0\nI/CdQQcWEbPfsAyz1u3W+zPAl4F/ohp1Y8y9tu8aaFQRMScMSZ7snChtbwY2A11nSoyImIx+vUc5\nHTILY0Q0Zja1ekdE9N1YY84wSKKMiMZ4SB5SJlFGRGP6McL5dEiijIhGVMOsNR1FPZMZuLdvJJ0n\naaOkGzscP1LSZkmry3Jmu3IRMZzsekvTmq5RfhL4EPCpLmW+ZfuFXY5HxFBSbr3rsP3NMhlQRMwx\nNmydAbXFOpquUdbxbEnXUc1p8Ubba8YXkLQEWAIwTztwwT03TXOIw2M7P7zpEGa8nbfP8Ku9bBr5\neV/O069nlJLOo5qNYePYQOOS3g78NfCLUuzNti8tx84AFgNbgdfZ/kq388/0RHkt8ATbWyQdC3yJ\natLyBynTVi4F2G7eTkPyb1RE9PH/rJ+k/WO8c2y/t3WHpAOBE4GnAY8Hvibpyba3djp5o405vdi+\nx/aWsn4psK2k3RsOKyL6oF+Ti0H1GA+oOwbFccD5tu+z/WOquXMO7faFGZ0oJT2uzLCGpEOp4r2z\n2agiol8m0Oq9u6RVLcuSmpd4bZlB9jxJu5Z9ewK3tZRZX/Z11Oitt6TPAkdS/QjrgbcB2wLY/ihw\nPPAaSSPAb4ATy8RAETHkqi6MtYtvsr1wgpf4CHBWudRZwPuAV9J+IrKukTTd6t11ZCLbH6J67hAR\ns9AgB8WwfcfYuqSPA5eUzfXA3i1F96JqLO5oRt96R8Qs5qrVu84yGZLmt2z+GTDWsWU5cKKk7SXt\nR9VAfFW3c830Vu+ImKX6ORVth8d4R0o6qFzmVuDVALbXSLoQuAkYAU7t1uINSZQR0aB+vUfZ4THe\nuV3Kvxt4d93zJ1FGRGOGpWk2iTIiGmFgJIkyIqK7IcmTSZQR0YxhGo8yiTIimjFDxpqsI4kyIhqT\nWRgjIrrIrXdERA0ZuDciooc8o4yI6MLkGWVERE+jQ1KlTKKMiMYMR5pMooyIhthma2qUERGd5fWg\niIga0pgTEdHDsEyBlakgIqIRY68H1Vl6KbMsbpR0Y8u+3SStkHRz+dy17JekD0paV2ZoPKTX+ZMo\nI6IxtmstNXwSOGbcvtOBy2wvAC4r2wAvoJonZwGwhGq2xq6SKCOiEdXAva619DyX/U3grnG7jwOW\nlfVlwItb9n/KlSuBXcZNRPYQSZQR0RjX/A/VpGGrWpYlNU7/WNu3A5TPx5T9ewK3tZRbX/Z1lMac\niGjMBFq9N9le2KfLqs2+rtXW1CgjohHGjNZcJumOsVvq8rmx7F8P7N1Sbi9gQ7cTJVFGRDNc9fWu\ns0zScmBRWV8EXNyy/+Wl9ftwYPPYLXonufWOiMa4T729JX0WOJLqWeZ64G3A2cCFkhYDPwVOKMUv\nBY4F1gG/Bl7R6/yNJUpJewOfAh5H9ahiqe0PjCsj4ANUf9SvgZNtXzvdsUZE/xkY6VPfHNsndTh0\nVJuyBk6dyPmbrFGOAG+wfa2kHYFrJK2wfVNLmdb3nQ6jet/psOkPNSL6z32rUQ5aY88obd8+Vju0\nfS+wloc20U/4faeIGA5Vz5yBNub0zYx4RilpX+BgYOW4Q53ed+r64DUihoBgVMMxLEbjiVLSo4DP\nA6fZvmf84TZfecg/L+Xl0yUA87RD32OMiMGYCbXFOhpNlJK2pUqSn7b9hTZFar3vZHspsBRgu3k7\nDccvHzHHGbOVrU2HUUtjzyhLi/a5wFrb7+9QbMLvO0XE8BjVaK2laU3WKI8AXgbcIGl12fdmYB8A\n2x9lEu87RcRwqHrmNJ8E62gsUdr+Nu2fQbaWmfD7ThExPJIoIyK6quqUwyCJMiIaYfJ6UERED2Yr\nv286iFqSKCOiEWnMiYioIYkyIqKr6pXzYZBEGRGNqAbFSI0yIqKrvB4UEdGF+9jqLelW4F5gKzBi\ne6Gk3YALgH2BW4G/sH33ZM6fOXMioiFm1FtrLTU9z/ZBLbM1ng5cZnsBcFnZnpQkyohoTN15GCfp\nOGBZWV8GvHiyJ0qijIiGVDffdRaqScNWtSxLHnIy+Kqka1qOPXZstLHy+ZjJRppnlBHRCAOjrl1b\n3NRyS93OEbY3SHoMsELSD6YcYIskyohohs2o+9OYY3tD+dwo6YvAocAdkubbvr3MtbVxsufPrXdE\nNGKsC2Od/3Qj6ZFlJlckPRL4I+BGqoG/F5Vii4CLJxtrapQR0RjXv/Xu5rHAF6tJE3gY8Bnb/yXp\nauBCSYuBnwInTPYCSZQR0ZD+dGG0fQvwzDb77wSOmvIFSKKMiAb1qUY5cEmUEdGQjHAeEdGVMaOj\nGbg3IqKr1CgjIrpxnlFGRPSQZ5QREV0ZcP2RgRqVRBkRDanGOB8GSZQR0RAz6pGmg6gliTIiGpQa\nZUREd0PS6t3Y6EGS9pZ0uaS1ktZIen2bMkdK2ixpdVnObCLWiBgED3qE875pskY5ArzB9rVliKRr\nJK2wfdO4ct+y/cIG4ouIgWs+CdbRWKIsQ7OPDdN+r6S1wJ7A+EQZEbOS88L5REjaFzgYWNnm8LMl\nXQdsAN5oe02b7y8BxubJuG/9lstuHFCok7U7sKnpIFoknu5mWjww82J6Sh/O8RUY2b1m2Ub/dtlu\n8vpIehTwDeDdtr8w7thOwKjtLZKOBT5Qpp7sdr5VPebWmHYzLabE091MiwdmXkwzLZ5Ba3QqCEnb\nAp8HPj0+SQLYvsf2lrJ+KbCtpLr/AkVE9EWTrd4CzgXW2n5/hzKPK+WQdChVvHdOX5QREc0+ozwC\neBlwg6TVZd+bgX0AbH8UOB54jaQR4DfAie79rGDpgOKdipkWU+LpbqbFAzMvppkWz0A1/owyImKm\ny3S1ERE9JFFGRPQw9IlS0m6SVki6uXzu2qHc1paukMsHEMcxkn4oaZ2k09sc317SBeX4yvLu6EDV\niOlkSb9o+V1eNcBYzpO0UVLbd1xV+WCJ9XpJhwwqlgnENG1daGt26Z3W3yjdjFvYHuoF+Gfg9LJ+\nOvCeDuW2DDCGecCPgP2B7YDrgAPHlfkb4KNl/UTgggH/LnViOhn40DT99/QHwCHAjR2OHwt8GRBw\nOLByBsR0JHDJNP0+84FDyvqOwH+3+e9rWn+jmjFN22/U5DL0NUrgOGBZWV8GvLiBGA4F1tm+xfbv\ngPNLXK1a47wIOGrs1acGY5o2tr8J3NWlyHHAp1y5EthF0vyGY5o2tm+3fW1ZvxcY69Lbalp/o5ox\nzQmzIVE+1lW/ccrnYzqU20HSKklXSup3Mt0TuK1lez0P/R/U/WVsjwCbgUf3OY6JxgTwknIbd5Gk\nvQcYTy91451uz5Z0naQvS3radFywS5fexn6jOt2Mp/M3mm4zoq93L5K+BjyuzaG3TOA0+9jeIGl/\n4OuSbrD9o/5ESLua4fj3ruqU6ac61/sP4LO275N0ClWN9/kDjKmb6f596rgWeIIf6EL7JaBrF9qp\nKl16Pw+cZvue8YfbfGXgv1GPmKb9N2rCUNQobR9t++ltlouBO8ZuP8rnxg7n2FA+bwGuoPrXsV/W\nA621sb2oBvFoW0bSw4CdGextX8+YbN9p+76y+XHgWQOMp5c6v+G08jR3oe3VpZcGfqN0M64MRaLs\nYTmwqKwvAi4eX0DSrpK2L+u7U/UK6udwblcDCyTtJ2k7qsaa8S3rrXEeD3zd5Wn4gPSMadzzrRdR\nPYNqynLg5aVl93Bg89gjlaZMZxfacp2uXXqZ5t+oTkzT+Rs1qunWpKkuVM/5LgNuLp+7lf0LgU+U\n9ecAN1C1/N4ALB5AHMdStQr+CHhL2fdO4EVlfQfgc8A64Cpg/2n4bXrF9E/AmvK7XA4cMMBYPks1\n/ujvqWpGi4FTgFPKcQEfLrHeACycht+nV0yvbfl9rgSeM8BYnkt1G309sLosxzb5G9WMadp+oyaX\ndGGMiOhhNtx6R0QMVBJlREQPSZQRET0kUUZE9JBEGRHRQxJlTJqkLeXz8ZIu6lH2NEmPmOD5j5R0\nyVRi7Od5Yu5KoowHkTRvot+xvcH28T2KnQZMKFFGzBRJlHOEpH0l/UDSspZBMB5Rjt0q6UxJ3wZO\nkPRESf8l6RpJ35J0QCm3n6TvSbpa0lnjzn1jWZ8n6b2SbijX+VtJrwMeD1wu6fJS7o/Kua6V9LnS\nn3hsDM0flFj+vMPfsrJ18AVJV0h6lqRDJX1X0vfL50Pmnpb0dklvbNm+sQz4gKSXSrpK1biKH5vM\nPxoxOyVRzi1PAZbafgZwD9UYmWN+a/u5ts+nmjjqb20/C3gj8P9KmQ8AH7H9P4Gfd7jGEmA/4OBy\nnU/b/iBVn+Tn2X5e6Ub6VuBo24cAq4C/l7QDVZ/zPwX+F+0HQoFqyLi/gPu7YT7e9jXAD4A/sH0w\ncCbwj3V/GElPBf43cITtg4CtwF/V/X7MbkMxelD0zW22v1PW/x14HfDesn0B3D9SzHOAz+mB4TK3\nL59HAC8p6/8GvKfNNY6mGqB4BMB2u4E/DgcOBL5TrrEd8D3gAODHtm8usfw7VeId70JgBfA2qoT5\nubJ/Z2CZpAVUXe+2bfcjdHAU1aAgV5eYHk6HAVZi7kminFvG91dt3f5V+dwG+GWpVdU5x3iqWWaF\n7ZMetFM6qMZ3sf0zSXdKegZVLfDV5dBZwOW2/6zcTl/R5usjPPhOaoeWmJbZPqPX9WPuya333LKP\npGeX9ZOAb48v4Gq8wR9LOgHun6flmeXwd6hGIYLOt6VfBU4pQ8khabey/16q6QSgGjzhCElPKmUe\nIenJVLfO+0l6YkuMnZwP/AOws+0byr6dgZ+V9ZM7fO9WqukfUDXnzH5l/2XA8ZIeMxa3pCd0uX7M\nIUmUc8taYJGk64HdgI90KPdXwGJJ11GNDDM2hcTrgVMlXU2VlNr5BPBT4Pry/b8s+5cCX5Z0ue1f\nUCWyz5ZYrqQauei3VLfa/1kac37S5W+5iCppX9iy75+Bf5L0Hao5g9r5PLCbpNXAa6hGV8L2TVTP\nTb9aYlpBNWdMREYPmivKregltp/ecCgRQyc1yoiIHlKjjIjoITXKiIgekigjInpIooyI6CGJMiKi\nhyTKiIge/j/x0CEwnSfVwAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1173a3a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.68      0.85      0.75       523\n",
      "    neutral       0.49      0.42      0.45       310\n",
      "   positive       0.56      0.29      0.39       180\n",
      "\n",
      "avg / total       0.60      0.62      0.59      1013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def plot_confusion_matrix(y, y_pred):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred),\n",
    "               cmap='inferno', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    plt.show()\n",
    "    \n",
    "print (\"classification accuracy:\", metrics.accuracy_score(label_test, y_test_prediction))\n",
    "plot_confusion_matrix(label_test, y_test_prediction)\n",
    "print(metrics.classification_report(label_test,y_test_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label well the negatives, but still label many neutral as negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
