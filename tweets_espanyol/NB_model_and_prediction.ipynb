{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airline_sentiment',\n",
       " 'is_reply',\n",
       " 'reply_count',\n",
       " 'retweet_count',\n",
       " 'text',\n",
       " 'tweet_coord',\n",
       " 'tweet_created',\n",
       " 'tweet_location',\n",
       " 'user_timezone']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_public.csv', encoding='utf-16', index_col='tweet_id', sep=',')\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weekdays = pd.to_datetime(df1.tweet_created).dt.dayofweek\n",
    "hoursday = pd.to_datetime(df1.tweet_created).dt.hour\n",
    "df1['weekday'] = pd.Series(weekdays, index=df1.index)\n",
    "df1['hoursday'] = pd.Series(hoursday, index=df1.index)\n",
    "weekday_names = []\n",
    "wn = ['lunes', 'martes', 'miercoles', 'jueves', 'viernes', 'sabado', 'domingo']\n",
    "for d in df1.weekday:\n",
    "    weekday_names.append(wn[d])\n",
    "df1['weekday_names'] = pd.Series(weekday_names, index=df1.index)\n",
    "df1.tweet_created = pd.to_datetime(df1.tweet_created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submit_file(df_submission, ypred):\n",
    "    date = datetime.datetime.now().strftime(\"%m_%d_%Y-%H_%M_%S\")\n",
    "    filename = 'submission_' + date + '.csv'\n",
    "\n",
    "    df_submission['airline_sentiment'] = ypred\n",
    "    df_submission[['airline_sentiment']].to_csv(filename)\n",
    "\n",
    "    print('Submission file created: {}'.format(filename))\n",
    "    print('Upload it to Kaggle InClass')\n",
    "\n",
    "def stemlematizer(df):\n",
    "    \"\"\" Function to reduce the words to their root\n",
    "    \"\"\"\n",
    "    lemmer=WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "    new_corpus1 = [' '.join([stemmer.stem(word) for word in tweet.split(' ')]) for tweet in df.text.values]\n",
    "    new_corpus = [' '.join([lemmer.lemmatize(word) for word in tweet.split(' ')]) for tweet in new_corpus1] \n",
    "    \n",
    "    df.text = new_corpus\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def load_and_train(dataframe, trainmodelclassifier, test=None, *model_args, **model_kwargs ):\n",
    "\n",
    "# If there is no test data, split the input\n",
    "    if test is None:\n",
    "        train, test = train_test_split(dataframe, test_size=0.15)\n",
    "    else:\n",
    "        train = dataframe\n",
    "\n",
    "    dataframe.airline_sentiment = pd.Categorical(dataframe.airline_sentiment)\n",
    "    \n",
    "    x_train = train['text']\n",
    "    y_train = train['airline_sentiment']\n",
    "    \n",
    "    x_test = test['text']\n",
    "    #we create the list of stop words specific for spanish:\n",
    "    spanish_stop_words = stopwords.words('spanish')\n",
    "    #we build the classifier, that has everything inside:\n",
    "    text_clf = Pipeline([('vect', CountVectorizer(min_df=1,\n",
    "                                              stop_words=spanish_stop_words, ngram_range=(1,2), \n",
    "                                              analyzer='word', token_pattern=r'[^@]\\b\\w+\\b')),\n",
    "                     ('tfidf', TfidfTransformer(norm='l2')),\n",
    "                     ('clf', trainmodelclassifier(*model_args, **model_kwargs))])\n",
    "\n",
    "    \n",
    "\n",
    "# SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None))])\n",
    "\n",
    "    text_clf.fit(x_train, y_train)\n",
    "    predicted = text_clf.predict(x_test)\n",
    "\n",
    "    try:\n",
    "        y_test = test['airline_sentiment'].values\n",
    "    except:\n",
    "        # It might be the submision file, where we don't have target values\n",
    "        y_test = None\n",
    "   \n",
    "    try:\n",
    "        acc = accuracy_score(test.airline_sentiment, predicted)\n",
    "        print('accuracy_score = ', acc)\n",
    "    except:\n",
    "        print('No accuracy computed beacuse there is NO test target')\n",
    "        acc = None\n",
    "    return predicted, x_train, y_train, x_test, y_test, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processTweet2(tweet):\n",
    "    # process the tweets\n",
    "\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #tweet = re.sub(r'\\#\\w+','',tweet)\n",
    "    return tweet  \n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    tweet = re.sub(r'CC:', ' EMOPOS ', tweet)\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMOPOS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMOPOS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMOPOS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMOPOS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMONEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMONEG ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # Remove punctuation\n",
    "    #word = word.strip('\\'\"?!,.():;')\n",
    "    word = word.strip('\\'\"?!,.')\n",
    "    # Convert more than 2 letter repetitions to 2 letter\n",
    "    # funnnnny --> funny\n",
    "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "    # Remove - & '\n",
    "    word = re.sub(r'(-|\\')', '', word)\n",
    "    # remove numbers\n",
    "    word = re.sub(r'\\d+','',word)\n",
    "    #remove users \n",
    "    word = re.sub(r'AT_USER','',word)\n",
    "    word = re.sub(r'URL','',word)\n",
    "    word = re.sub(r'rt','',word)\n",
    "    word = re.sub(r'via','',word)\n",
    "    return word\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = [handle_emojis(tweet) for tweet in df1.text.values]\n",
    "tweets2 = [processTweet2(tweet) for tweet in tweets]\n",
    "tweets3 = [' '.join(preprocess_word(word) for word in tweet.split(' ')) for tweet in tweets2]\n",
    "df1.text = tweets3\n",
    "df1.text.values\n",
    "df1 = stemlematizer(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hh = ['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']\n",
    "hh_names = ['zero', 'una', 'dos', 'tres', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve', 'diez', 'once', 'doce', 'trece', 'catorce', 'quince', 'dieciseis', 'diecisiete', 'dieciocho', 'diecinueve', 'veinte', 'veintiuno', 'veintios', 'veintitres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we create the list of stop words specific for spanish:\n",
    "spanish_stop_words = stopwords.words('spanish')\n",
    "count_vect = CountVectorizer(min_df=1, stop_words=newstopWords, ngram_range=(1,2), analyzer='word') #, token_pattern=r'[^@]\\b\\w+\\b')\n",
    "X_train_counts = count_vect.fit_transform(df1.text.values)\n",
    "voc = count_vect.vocabulary_\n",
    "X_train_counts = X_train_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in hh_names:\n",
    "    ind = df1.query('hoursday == @i').index\n",
    "    df1.loc[ind,i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for h in hh_names:\n",
    "    col = np.array(df1[h].fillna(value = 0))\n",
    "    np.column_stack((X_train_counts,col))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.unique(np.array(df1['weekday_names'])):\n",
    "    ind = df1.query('weekday_names == @i').index\n",
    "    df1.loc[ind,i] = 1\n",
    "for d in wn:\n",
    "    col = np.array(df1[d].fillna(value = 0))\n",
    "    np.column_stack((X_train_counts,col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.613039796782\n",
      "0.602878916173\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "features_train,features_test,label_train,label_test = \\\n",
    "train_test_split(X_train_tfidf, df1.airline_sentiment.values,test_size=0.15,random_state = 123)\n",
    "sgc = SGDClassifier(penalty = 'elasticnet',max_iter = 10)\n",
    "svm = LinearSVC()\n",
    "sgc.fit(features_train,label_train)\n",
    "svm.fit(features_train,label_train)\n",
    "score = sgc.score(features_test,label_test)\n",
    "scoresvm = svm.score(features_test,label_test)\n",
    "print(score)\n",
    "print(scoresvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I do the processing of the sampleSubmission.csv file, with the created model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df0_s = pd.read_csv('tweets_submission.csv', index_col='tweet_id', sep=',')\n",
    "# Keep relevant columns only\n",
    "df_s = df0_s.drop(['retweet_count','is_reply','reply_count','tweet_coord','tweet_location','user_timezone'], axis=1)\n",
    "#df.head()\n",
    "df_s['text'] = df_s['text'].apply(basicCleaning)\n",
    "df_s['airline'] = df_s['text'].apply(extract_airline)\n",
    "# From 'airline' column create a one-hot encoding matrix for airline name\n",
    "tweet_airline = df_s['airline'].str.split(r'|', expand=True).stack().reset_index(level='tweet_id')\n",
    "tweet_airline.columns = ['tweet_id','airline']\n",
    "tweet_airline = tweet_airline.set_index('tweet_id')\n",
    "# One-hot encoding for airline name\n",
    "onehot = pd.get_dummies(tweet_airline['airline'])\n",
    "df_airlines = onehot.groupby('tweet_id').sum()\n",
    "\n",
    "# Concatenate the two dataframes (original+one-hot airlines) and delete the airline column\n",
    "merged = pd.concat([df_s, df_airlines], axis=1, join_axes=[df_s.index])\n",
    "df1_s = merged.drop('airline', axis=1)\n",
    "df1_s['text'] = df1_s['text'].apply(moreCleaning)\n",
    "\n",
    "weekdays = pd.to_datetime(df1_s.tweet_created).dt.dayofweek\n",
    "hoursday = pd.to_datetime(df1_s.tweet_created).dt.hour\n",
    "df1_s['weekday'] = pd.Series(weekdays, index=df1_s.index)\n",
    "df1_s['hoursday'] = pd.Series(hoursday, index=df1_s.index)\n",
    "weekday_names = []\n",
    "wn = ['lunes', 'martes', 'miercoles', 'jueves', 'viernes', 'sabado', 'domingo']\n",
    "for d in df1_s.weekday:\n",
    "    weekday_names.append(wn[d])\n",
    "df1_s['weekday_names'] = pd.Series(weekday_names, index=df1_s.index)\n",
    "df1_s.tweet_created = pd.to_datetime(df1_s.tweet_created)\n",
    "\n",
    "\n",
    "\n",
    "tweets = [handle_emojis(tweet) for tweet in df1_s.text.values]\n",
    "tweets2 = [processTweet2(tweet) for tweet in tweets]\n",
    "tweets3 = [' '.join(preprocess_word(word) for word in tweet.split(' ')) for tweet in tweets2]\n",
    "df1_s.text = tweets3\n",
    "df1_s.text.values\n",
    "df1_s = stemlematizer(df1_s)\n",
    "\n",
    "count_vect = CountVectorizer(vocabulary=voc, min_df=1, stop_words=newstopWords, ngram_range=(1,2), analyzer='word')\n",
    "X_submission_counts = count_vect.fit_transform(df1_s.text.values)\n",
    "X_submission_counts = X_submission_counts.toarray()\n",
    "\n",
    "for i in hh_names:\n",
    "    ind = df1_s.query('hoursday == @i').index\n",
    "    df1_s.loc[ind,i] = 1\n",
    "\n",
    "for h in hh_names:\n",
    "    col = np.array(df1_s[h].fillna(value = 0))\n",
    "    np.column_stack((X_submission_counts,col))    \n",
    "    \n",
    "for i in np.unique(np.array(df1_s['weekday_names'])):\n",
    "    ind = df1_s.query('weekday_names == @i').index\n",
    "    df1_s.loc[ind,i] = 1\n",
    "for d in wn:\n",
    "    col = np.array(df1_s[d].fillna(value = 0))\n",
    "    np.column_stack((X_submission_counts,col))\n",
    "\n",
    "X_submission_tfidf = tfidf_transformer.transform(X_submission_counts)\n",
    "#features_train,features_test,label_train,label_test = \\\n",
    "#train_test_split(X_submission_tfidf, df1_s.airline_sentiment.values,test_size=1.00,random_state = 123)\n",
    "#sgc = SGDClassifier(penalty = 'elasticnet',max_iter = 10)\n",
    "#svm = LinearSVC()\n",
    "#sgc.fit(features_train,label_train)\n",
    "#svm.fit(features_train,label_train)\n",
    "prediction_sgc = sgc.predict(X_submission_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submit_file(df0_s, prediction_sgc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
